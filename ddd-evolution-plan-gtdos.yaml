# DDD Evolution Plan
# Generated by: /ddd-evolve
# Date: 2026-02-21T12:00:00Z
# Input: /Users/mhcandan/dev/gtdos/specs/shortfalls.yaml
# Status: pending_review

meta:
  projects_analyzed: 1
  total_shortfalls_reviewed: 28
  status: applied
  reviewed_at: "2026-02-21T14:30:00Z"
  applied_at: "2026-02-21T14:45:00Z"
  verdicts:
    REAL_GAP: 9
    ENHANCEMENT: 4
    VAGUE: 0
    ALREADY_POSSIBLE: 7
    BY_DESIGN: 2
    PROJECT_SPECIFIC: 1
  decisions:
    approved: 13
    deferred: 0
    rejected: 0
    accepted_verdict: 10

# ──────────────────────────────────────────────
# TIER 1: Real gaps — recommended for action
# ──────────────────────────────────────────────
real_gaps:
  - id: "RG-001"
    shortfall: "data_store node has no aggregate operation"
    verdict: REAL_GAP
    evidence:
      frequency: 1
      projects: ["gtdos"]
      max_severity: high
      specificity: specific
      workaround_quality: lossy_workaround
    analysis: |
      Database-level aggregations (COUNT, SUM, AVG, GROUP BY) are essential for any
      stats or reporting flow. The current spec has no way to express these — every
      stats computation requires a process node that obscures the operation on the
      canvas. The collection:aggregate node works on in-memory arrays only; it cannot
      push aggregations to the database, forcing an expensive read-all-then-aggregate
      pattern. This affects 4 flows in gtdos (get-inbox-stats, get-signal-stats,
      get-dashboard-data, get-weekly-summary) and will appear in virtually every
      data-driven project that needs reporting or dashboards.
    recommendation:
      action: ADD_FIELD
      scope: |
        Add operation: aggregate to data_store node with:
          aggregate_fields: [{function: count|sum|avg|min|max, field, alias}]
          group_by: [field names]
        Also add expression support in query conditions for dynamic date arithmetic:
          query: { updated_at: { lt: { expr: 'now - $.stale_days * 86400000' } } }
      affects:
        spec: true
        commands: true
        tool: true
        validator: true
      effort: medium
      risk: low
      breaking: false
    decision: approve

  - id: "RG-002"
    shortfall: "parallel node has no best_effort failure policy"
    verdict: REAL_GAP
    evidence:
      frequency: 1
      projects: ["gtdos"]
      max_severity: high
      specificity: specific
      workaround_quality: lossy_workaround
    analysis: |
      The parallel node join options (all | any | n_of) don't cover the common
      "run all branches, collect whatever succeeded, continue" pattern. join:all fails
      if any branch errors. join:any fires after the first success only. join:n_of
      requires a fixed count. None expresses best_effort: "try all branches, aggregate
      successes into done, continue regardless of individual failures." For dashboard
      flows with 4 independent data sources, a failing social feed shouldn't block
      the inbox count — this is exactly the best_effort pattern.
    recommendation:
      action: ADD_FIELD
      scope: |
        Add failure_policy field to parallel node:
          failure_policy: all_required | any_required | best_effort
          (default: all_required for backwards compatibility)
        When best_effort: done handle always fires; $.branch_errors contains
        per-branch error info. This complements (not replaces) the join field.
      affects:
        spec: true
        commands: true
        tool: true
        validator: false
      effort: small
      risk: low
      breaking: false
    decision: approve

  - id: "RG-003"
    shortfall: "service_call has no built-in oauth_config for token lifecycle"
    verdict: REAL_GAP
    evidence:
      frequency: 1
      projects: ["gtdos"]
      max_severity: high
      specificity: specific
      workaround_quality: lossy_workaround
    analysis: |
      OAuth2 token refresh is a universal pattern for any project integrating with
      third-party APIs (Gmail, Microsoft, Salesforce, GitHub, etc.). Without a
      first-class oauth_config on service_call, every authenticated flow requires an
      identical process node that decrypts stored tokens, checks expiry, and refreshes
      via the token endpoint — then re-encrypts and stores the new tokens. This pattern
      is duplicated verbatim in every OAuth-connected flow and is invisible on the
      canvas. The system.yaml integrations section already defines OAuth auth config;
      service_call should be able to reference it for automatic token lifecycle.
    recommendation:
      action: ADD_FIELD
      scope: |
        Add oauth_config field to service_call:
          oauth_config:
            token_store: "{data_store reference for stored tokens}"
            refresh_url: string
            client_id_env: string
            client_secret_env: string
        When set, /ddd-implement generates automatic token refresh before the call.
        Alternatively, allow inheritance from a system.yaml integration reference
        (integration: gmail_api) when that integration defines oauth auth.
      affects:
        spec: true
        commands: true
        tool: false
        validator: false
      effort: medium
      risk: low
      breaking: false
    decision: approve

  - id: "RG-004"
    shortfall: "collection node cannot join or cross-reference two arrays"
    verdict: REAL_GAP
    evidence:
      frequency: 1
      projects: ["gtdos"]
      max_severity: medium
      specificity: specific
      workaround_quality: lossy_workaround
    analysis: |
      The collection node operates on a single input array. Cross-referencing two
      arrays — finding items in A not in B, or enriching A with fields from B by
      key — is impossible without a process node. In gtdos this occurs in
      check-project-health (projects with no matching next_action) and
      get-dashboard-data (enrich projects with next_action counts). These are pure
      data operations that deserve structured spec representation. The merge operation
      only concatenates; it does not join by key.
    recommendation:
      action: ADD_FIELD
      scope: |
        Add join operation to collection node:
          operation: join
          left: "$.array_a"
          right: "$.array_b"
          on: "item.id === right.project_id"
          type: inner | left | anti   # anti = items in left not matching right
          output: "enriched_projects"
        Also extend filter predicate to support cross-collection references:
          predicate: "!$.next_actions.some(a => a.project_id === item.id)"
      affects:
        spec: true
        commands: true
        tool: true
        validator: true
      effort: medium
      risk: low
      breaking: false
    decision: approve

  - id: "RG-005"
    shortfall: "form spec has no auto-save configuration"
    verdict: REAL_GAP
    evidence:
      frequency: 1
      projects: ["gtdos"]
      max_severity: medium
      specificity: specific
      workaround_quality: lossy_workaround
    analysis: |
      Auto-save (debounced save on change, no explicit submit button) is a standard
      UX pattern for document editors, settings forms, profile pages, and any
      long-form editing. The current FormSpec only models a submit button via the
      submit field; there's no way to declare auto-save intent. /ddd-implement
      generates a submit button regardless, losing a meaningful UI contract. This
      is not gtdos-specific — any CMS, wiki, note-taking, or settings-heavy app
      will need this pattern.
    recommendation:
      action: ADD_FIELD
      scope: |
        Add optional auto_save field to FormSpec:
          auto_save:
            debounce_ms: number
            flow: "domain/flow-id"
            key_field: string?   # field to use as save key for optimistic updates
        When set: no submit button rendered; debounced useEffect saves after
        inactivity; 'Saving...' / 'Saved' / 'Error' status indicator shown.
      affects:
        spec: true
        commands: true
        tool: false
        validator: false
      effort: small
      risk: low
      breaking: false
    decision: approve

  - id: "RG-006"
    shortfall: "item-list component has no group_by for sectioned lists"
    verdict: REAL_GAP
    evidence:
      frequency: 1
      projects: ["gtdos"]
      max_severity: medium
      specificity: specific
      workaround_quality: lossy_workaround
    analysis: |
      Grouped/sectioned lists with section headers and item counts are one of the most
      common UI patterns in data-heavy apps. Without group_by on item-list, this
      requires either separate list sections per group (verbose, duplicates data_source
      refs) or a process node that pre-groups data (not a UI concern). This pattern
      appears in virtually any app with categorized content — task managers, inboxes,
      product catalogs, issue trackers.
    recommendation:
      action: ADD_FIELD
      scope: |
        Add optional group_by field to item-list PageSection:
          group_by:
            field: "$.category"          # partition key
            label_field: "$.category_label"   # optional display label
            show_count: true             # show item count per section
            collapsible: false           # allow collapse/expand per section
        Items are partitioned by the group field value and rendered with sticky
        section headers between groups.
      affects:
        spec: true
        commands: true
        tool: false
        validator: false
      effort: small
      risk: low
      breaking: false
    decision: approve

  - id: "RG-007"
    shortfall: "UI page specs have no keyboard_shortcuts field"
    verdict: REAL_GAP
    evidence:
      frequency: 1
      projects: ["gtdos"]
      max_severity: medium
      specificity: specific
      workaround_quality: lossy_workaround
    analysis: |
      Keyboard shortcuts are declared on flow metadata (flow.keyboard_shortcut) and
      trigger specs (event: "shortcut Cmd+K") but there is no corresponding field in
      UI page specs to declare that a page registers a shortcut handler. The shortcut
      only exists at the backend layer; /ddd-implement has no way to generate the
      frontend key listener because the UI spec doesn't declare it. The connection
      between trigger spec and UI key registration is implicit and unspecifiable.
    recommendation:
      action: ADD_FIELD
      scope: |
        Add optional keyboard_shortcuts array to PageSpec:
          keyboard_shortcuts:
            - keys: "Cmd+N"
              label: "Quick capture"
              action:
                type: call_flow | navigate | toggle
                flow: "capture/quick-capture"
                args: {}
        /ddd-implement generates a useEffect with document keydown listeners.
        Also add to pages.yaml top-level for global (cross-page) shortcuts.
      affects:
        spec: true
        commands: true
        tool: false
        validator: false
      effort: small
      risk: low
      breaking: false
    decision: approve

  - id: "RG-008"
    shortfall: "No markdown-editor form field type"
    verdict: REAL_GAP
    evidence:
      frequency: 1
      projects: ["gtdos"]
      max_severity: medium
      specificity: specific
      workaround_quality: lossy_workaround
    analysis: |
      Long-form markdown editing with preview is a distinct UX pattern from plain
      textarea. The current form field types include textarea but not a markdown-aware
      editor type. Without it, /ddd-implement generates a plain textarea for markdown
      content — missing preview mode entirely. This applies to any CMS, wiki, note-
      taking app, documentation tool, or settings page with rich text content.
    recommendation:
      action: ADD_FIELD
      scope: |
        Add markdown as a valid FormField type:
          type: markdown
          markdown_config:           # optional
            mode: toggle | split     # default: toggle
            toolbar: true
            min_height: 300
        /ddd-implement generates an appropriate editor component (e.g.,
        @uiw/react-md-editor or similar) with edit/preview toggle.
      affects:
        spec: true
        commands: true
        tool: false
        validator: false
      effort: small
      risk: low
      breaking: false
    decision: approve

  - id: "RG-009"
    shortfall: "No conditional (skip-on-condition) connections between nodes"
    verdict: REAL_GAP
    evidence:
      frequency: 1
      projects: ["gtdos"]
      max_severity: medium
      specificity: specific
      workaround_quality: lossy_workaround
    analysis: |
      Optional processing steps — write a history record only if fields changed,
      send a notification only if the user opted in — currently require a full
      decision node with true/false branches wired to terminals or downstream nodes.
      This creates visual noise for simple guards and forces every optional step to
      fork into two explicit paths. A conditional edge eliminates decision nodes used
      purely as single-step guards.
    recommendation:
      action: ADD_CONNECTION_FEATURE
      scope: |
        Add optional condition field to connection objects:
          connections:
            - targetNodeId: data_store-createHistory
              condition: "$.changed_fields.length > 0"
              sourceHandle: success
        When condition evaluates to false at runtime, this edge is skipped.
        Decision nodes remain the right choice when both branches have meaningful
        downstream logic — this is for single-step optional processing only.
      affects:
        spec: true
        commands: true
        tool: true
        validator: false
      effort: medium
      risk: medium
      breaking: false
    decision: approve

# ──────────────────────────────────────────────
# TIER 2: Enhancements — nice to have
# ──────────────────────────────────────────────
enhancements:
  - id: "EN-001"
    shortfall: "stat-card has no trend indicator field"
    verdict: ENHANCEMENT
    evidence:
      frequency: 1
      projects: ["gtdos"]
      max_severity: low
      specificity: specific
      workaround_quality: adequate_workaround
    analysis: |
      Trend indicators (inbox +8 from yesterday) are standard dashboard UX.
      Workaround: include trend data in subtitle as plain text. Functional but
      unstructured — /ddd-implement can't generate typed trend logic from it.
    recommendation:
      action: ADD_FIELD
      scope: |
        Add optional trend field to stat-card sections:
          trend:
            value: "$.previous_count"
            direction: auto | up | down
            format: delta | percent | raw
      effort: small
    decision: approve

  - id: "EN-002"
    shortfall: "parallel branches have no explicit output_key for result namespacing"
    verdict: ENHANCEMENT
    evidence:
      frequency: 1
      projects: ["gtdos"]
      max_severity: high
      specificity: specific
      workaround_quality: adequate_workaround
    analysis: |
      Parallel branches write to the shared flow context ($). Authors must manually
      use distinct variable names per branch to avoid collisions — this works but
      is implicit. An explicit output_key field would make the contract visible on
      the canvas and allow the DDD Tool to show what each branch produces.
    recommendation:
      action: ADD_FIELD
      scope: |
        Add optional output_key to ParallelBranch:
          branches:
            - id: inbox
              label: "Fetch inbox items"
              output_key: inbox_result
        /ddd-implement uses output_key as the $ variable name for that branch result.
      effort: small
    decision: approve

  - id: "EN-003"
    shortfall: "data_store returning field undocumented for single create/update"
    verdict: ENHANCEMENT
    evidence:
      frequency: 1
      projects: ["gtdos"]
      max_severity: medium
      specificity: moderate
      workaround_quality: adequate_workaround
    analysis: |
      The Usage Guide documents returning: true only for bulk operations
      (create_many, update_many, delete_many). For single create/update, the behavior
      is implicit (ORM returns the record). Spec authors rely on convention without
      a way to express this intent. Documentation clarity fix, not a new feature.
    recommendation:
      action: UPDATE_SPEC
      scope: |
        Clarify in the Usage Guide that returning: true is valid on all operation types.
        Add example for single create with returning:
          spec:
            operation: create
            model: GtdItem
            data: "$.fields"
            returning: true
      effort: small
    decision: approve

  - id: "EN-004"
    shortfall: "llm_call structured_output cannot ref shared/types.yaml enums"
    verdict: ENHANCEMENT
    evidence:
      frequency: 1
      projects: ["gtdos"]
      max_severity: low
      specificity: specific
      workaround_quality: adequate_workaround
    analysis: |
      Enum values in structured_output must be duplicated inline even when they're
      already defined in shared/types.yaml. A DRY violation — if categories change,
      the llm_call spec must be manually updated. Workaround works but creates drift.
    recommendation:
      action: ADD_FIELD
      scope: |
        Allow ref syntax in structured_output property definitions:
          structured_output:
            suggestedCategory:
              type: string
              ref: gtd_category    # resolved from shared/types.yaml
        /ddd-implement resolves the ref and injects enum values into the schema.
      effort: small
    decision: approve

# ──────────────────────────────────────────────
# TIER 3: Not actionable
# ──────────────────────────────────────────────
not_actionable:
  - id: "NA-001"
    shortfall: "Trigger-level event payload filtering"
    verdict: ALREADY_POSSIBLE
    reason: |
      The trigger spec already has a filter field (Section 6.1 of the Usage Guide):
        spec:
          event: "event:InboxItemProcessed"
          filter:
            "payload.source_type": web
      Supports dot notation and operators. The gtdos report used decision nodes
      instead — but trigger filter was already available.
    decision: accept_verdict

  - id: "NA-002"
    shortfall: "Event emit nodes have no fire-and-forget option"
    verdict: ALREADY_POSSIBLE
    reason: |
      The event node already has async: boolean (Section 6.2). Setting async: true
      makes the emit fire-and-forget — the flow continues immediately without
      waiting for consumer acknowledgment. The gtdos report missed this field.
    decision: accept_verdict

  - id: "NA-003"
    shortfall: "No per-flow rate limiting declaration"
    verdict: ALREADY_POSSIBLE
    reason: |
      The trigger spec already has rate_limit: { window_ms, max_requests, key_by,
      on_exceeded } (Section 6.1). This is per-trigger (effectively per-flow) and
      /ddd-implement generates Express middleware from it.
    decision: accept_verdict

  - id: "NA-004"
    shortfall: "No page_fetch node for web page metadata extraction"
    verdict: ALREADY_POSSIBLE
    reason: |
      service_call (to fetch the URL) + parse:html (to extract metadata) covers this
      completely. The parse node supports html format and extracts structured data.
      A specialized page_fetch node would duplicate existing two-node functionality.
    decision: accept_verdict

  - id: "NA-005"
    shortfall: "No drag-drop reordering in item-list"
    verdict: ALREADY_POSSIBLE
    reason: |
      The Usage Guide documents interactions.reorder on item-list (Section 4.7):
        interactions:
          - pattern: reorder
            update_flow: domain/flow-id
      Already in the spec. The gtdos report incorrectly identified this as missing.
    decision: accept_verdict

  - id: "NA-006"
    shortfall: "collection:aggregate doesn't support multi-field aggregations"
    verdict: ALREADY_POSSIBLE
    reason: |
      Complex accumulator expressions support multi-field aggregation:
        accumulator:
          init: { count: 0, total_score: 0, ai_suggested: 0 }
          expression: |
            { count: acc.count + 1,
              total_score: acc.total_score + item.score,
              ai_suggested: acc.ai_suggested + (item.ai_suggested ? 1 : 0) }
      This is a documentation awareness gap. Note: database-level aggregate in
      data_store is still a real gap (RG-001) — this addresses in-memory arrays only.
    decision: accept_verdict

  - id: "NA-007"
    shortfall: "soft_delete cross-cutting concern has no first-class representation"
    verdict: ALREADY_POSSIBLE
    reason: |
      architecture.yaml cross_cutting_patterns is exactly for this:
        cross_cutting_patterns:
          soft_delete:
            description: "All reads exclude soft-deleted records"
            used_by_domains: [users, gtd-engine, inbox]
            convention: "Add deleted_at: null to all data_store read queries"
      /ddd-implement applies the convention. Enforcement is intentionally
      convention-based — DDD prefers explicit specs over implicit ORM magic.
    decision: accept_verdict

  - id: "NA-008"
    shortfall: "connection data annotations are not validated"
    verdict: BY_DESIGN
    reason: |
      The Usage Guide explicitly states data annotations are "purely descriptive —
      it doesn't affect behavior but enables data shape hover in the DDD Tool."
      Full validation would require type inference across the flow graph — high
      complexity for limited benefit. Intentionally decorative.
    decision: accept_verdict

  - id: "NA-009"
    shortfall: "guided-stepper UI component for sequential checklists"
    verdict: BY_DESIGN
    reason: |
      A guided-stepper is GTD-review-specific. The item-list component with
      item_actions (Complete/Skip buttons) already covers sequential workflows
      at spec level. DDD's component set is intentionally minimal and composable
      — complex patterns are expressed through combinations of existing components.
    decision: accept_verdict

  - id: "NA-010"
    shortfall: "shell_exec node type for subprocess / osascript execution"
    verdict: PROJECT_SPECIFIC
    reason: |
      Shell/subprocess execution is a macOS desktop app pattern with very limited
      generalizability — most DDD projects are web or mobile apps. For Tauri apps,
      ipc_call to a Rust command is the correct DDD pattern (Rust handles subprocess).
      For Node.js desktop apps, a process node accurately describes the intent.
      The process node IS the right abstraction — DDD describes what, not how.
    decision: accept_verdict

# ──────────────────────────────────────────────
# Execution order (for approved items only)
# ──────────────────────────────────────────────
recommended_order:
  - phase: 1
    ids: ["RG-002", "RG-005", "RG-006", "RG-007", "RG-008"]
    rationale: >
      Pure spec + commands field additions. No tool changes needed for most.
      Low risk, high impact, no dependencies between them.

  - phase: 2
    ids: ["RG-001", "RG-003", "EN-003"]
    rationale: >
      RG-001 (data_store aggregate) requires more design and tool/validator changes.
      RG-003 (oauth_config) needs implementation design decisions. EN-003 is
      documentation-only. All independent of Phase 1.

  - phase: 3
    ids: ["RG-004", "RG-009"]
    rationale: >
      collection:join (RG-004) requires new operation semantics and validator support.
      Connection conditions (RG-009) require DDD Tool canvas changes (conditional
      edge rendering). Both are more complex and depend on Phase 1+2 being stable.

  - phase: 4
    ids: ["EN-001", "EN-002", "EN-004"]
    rationale: >
      Low-priority enhancements. Apply after Phase 1-3 are settled.
