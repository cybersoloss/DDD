# DDD Tool — Usage Guide for Claude

This guide teaches you how to create and manage a DDD (Design Driven Development) project using the DDD Tool + Claude Code terminal workflow. You will write YAML spec files that the DDD Tool reads, and use `/ddd-implement` to generate implementation code.

---

## 1. Project Structure

A DDD project has this directory layout:

```
my-project/
  ddd-project.json              # Project config (domains list)
  specs/
    system.yaml                  # Tech stack, project identity
    architecture.yaml            # Conventions, API design, cross-cutting patterns
    config.yaml                  # Environment variable schema
    infrastructure.yaml          # Services, ports, startup order, deployment
    system-layout.yaml           # L1 canvas positions (auto-managed by DDD Tool)
    shared/
      errors.yaml                # Error codes with HTTP status mappings
      types.yaml                 # Shared enums and value objects
    schemas/
      _base.yaml                 # Base model (id, timestamps, soft delete)
      {model}.yaml               # Data model definitions (fields, indexes, seed)
    ui/
      pages.yaml                 # Page registry, navigation, app type
      {page-id}.yaml             # Per-page spec: sections, components, data bindings
    domains/
      {domain-id}/
        domain.yaml              # Domain config: flows, events, layout
        flows/
          {flow-id}.yaml         # Flow spec: trigger, nodes, connections
  .ddd/
    mapping.yaml                 # Implementation tracking (specHash, files)
    annotations/                 # Implementation wisdom captured by /ddd-reflect
    autosave/                    # Crash recovery
    reconciliations/             # Reconciliation reports from /ddd-sync
  src/                           # Implementation code (generated by Claude Code)
```

---

## 2. ddd-project.json

The root project config. Lists all domains:

```json
{
  "domains": [
    { "name": "Users", "description": "User management and authentication" },
    { "name": "Billing", "description": "Subscriptions and payments" },
    { "name": "Support", "description": "Customer support tickets" }
  ]
}
```

Each domain entry can include an optional `role` field:

```json
{ "name": "Users", "description": "User management and authentication", "role": "entity" }
```

Optional `role` values: `entity` (CRUD/data), `process` (automated workflows), `interface` (human-facing). Used by DDD Tool for visual differentiation at L1.

Domain IDs are derived from names: lowercase, spaces replaced with hyphens (e.g., "Users" -> "users").

---

## 3. Domain YAML

**Path:** `specs/domains/{domain-id}/domain.yaml`

```yaml
name: Users
description: User management and authentication
owns_schemas: ["User", "Session"]
stores:
  - name: auth-store
    shape: { currentUser: "User | null", token: "string | null" }
    access_pattern: read_write
flows:
  - id: user-register
    name: User Registration
    description: Register a new user account
    type: traditional
  - id: user-login
    name: User Login
    description: Authenticate and issue JWT
    type: traditional
publishes_events:
  - event: UserRegistered
    schema: User
    from_flow: user-register
    description: Fired after successful registration
  - event: UserLoggedIn
    from_flow: user-login
consumes_events: []
layout:
  flows:
    user-register: { x: 100, y: 100 }
    user-login: { x: 100, y: 300 }
  portals: {}
```

### Domain Fields

| Field | Type | Description |
|-------|------|-------------|
| `name` | string | Display name |
| `description` | string? | What this domain handles |
| `flows` | DomainFlowEntry[] | List of flows in this domain |
| `publishes_events` | EventWiring[] | Events this domain emits |
| `consumes_events` | EventWiring[] | Events this domain listens to |
| `owns_schemas` | string[]? | Schema names this domain owns (e.g., ["User", "Session"]) |
| `stores` | StoreDefinition[]? | In-memory state store declarations for this domain |
| `on_error` | DomainOnError? | Domain-level error hook — `/ddd-implement` adds this behavior to all error terminals in the domain |
| `event_groups` | EventGroup[]? | Named groups of events for use in multi-event triggers |
| `groups` | FlowGroup[]? | Visual grouping of flows at L2 |
| `layout` | DomainLayout | Canvas positions (managed by DDD Tool) |

### FlowGroup

| Field | Type | Description |
|-------|------|-------------|
| `id` | string | Group ID |
| `name` | string | Display name |
| `flows` | string[] | Flow IDs in this group |

### EventGroup

Named collections of events for use in multi-event triggers. Instead of listing 20+ events on a trigger, reference a group with `event_group:{name}` in the trigger's `event` field.

| Field | Type | Description |
|-------|------|-------------|
| `name` | string | Group name (referenced as `event_group:{name}` in triggers) |
| `description` | string? | What this group represents |
| `events` | string[] | Event names in this group |

```yaml
# Example: define event groups in domain.yaml
event_groups:
  - name: audit_events
    description: "All events that should be logged for audit"
    events:
      - PostCreated
      - PostUpdated
      - PostDeleted
      - UserLoggedIn
      - UserLoggedOut
      - SettingsUpdated

# Then reference in a trigger:
# event: "event_group:audit_events"
```

### StoreDefinition

Declares an in-memory state store owned by this domain. Enables validation of `data_store` memory references and store topology visualization at L1/L2.

| Field | Type | Description |
|-------|------|-------------|
| `name` | string | Store name (referenced by `data_store` nodes with `store_type: memory`) |
| `shape` | `Record<string, string>`? | Field names and types (e.g., `{ domains: "DomainConfig[]", currentFlow: "FlowDocument" }`) |
| `initial_state` | `Record<string, unknown>`? | Default/initial values |
| `selectors` | string[]? | Common selector paths (e.g., `["domains", "currentFlow.nodes"]`) |
| `access_pattern` | `'read_write' \| 'read_only'`? | Whether other domains can write to this store (default: `'read_write'`) |

```yaml
# Example: declare Zustand stores in a domain
stores:
  - name: project-store
    shape:
      domains: "Record<string, DomainConfig>"
      projectPath: "string"
      systemLayout: "SystemLayout"
    selectors: ["domains", "projectPath", "systemLayout.zones"]
    access_pattern: read_write
  - name: ui-store
    shape:
      minimapVisible: "boolean"
      locked: "boolean"
    access_pattern: read_write
```

### DomainOnError

Domain-level error hook. When set, `/ddd-implement` automatically adds this behavior to all error terminals in the domain, reducing boilerplate for centralized error handling patterns (e.g., error toast systems).

| Field | Type | Description |
|-------|------|-------------|
| `emit_event` | string? | Event name to emit from all error terminals (e.g., `"ErrorOccurred"`) |
| `description` | string? | What the error hook does |

```yaml
# Example: all error terminals in this domain emit ErrorOccurred
on_error:
  emit_event: ErrorOccurred
  description: "Emit error event for centralized toast display"
```

### DomainFlowEntry

| Field | Type | Description |
|-------|------|-------------|
| `id` | string | Unique flow ID (kebab-case) |
| `name` | string | Display name |
| `description` | string? | What this flow does |
| `type` | `'traditional' \| 'agent'` | Flow type |
| `tags` | string[]? | Custom tags (e.g., ["cron", "internal", "public-api"]) |
| `keyboard_shortcut` | string? | Keyboard shortcut (e.g., `"Cmd+K"`). Auto-populated from `shortcut` trigger events by DDD Tool. Shown as badge at L2. |
| `criticality` | string? | `'critical' \| 'high' \| 'normal' \| 'low'` — visual priority at L2 |
| `throughput` | string? | Expected volume (e.g., `"~500 items/day"`) — shown as subtitle at L2 |
| `template` | string? | Reference to a template flow ID (see Parameterized Flow pattern in Section 16) |
| `parameters` | `Record<string, unknown>`? | Parameter values for template instantiation (required when `template` is set) |

### EventWiring

| Field | Type | Description |
|-------|------|-------------|
| `event` | string | Event name (e.g., UserRegistered) |
| `schema` | string? | Data schema reference |
| `from_flow` | string? | Which flow publishes it |
| `handled_by_flow` | string? | Which flow consumes it |
| `description` | string? | What this event means |
| `payload` | `Record<string, unknown>?` | Event data shape (field names and types) |

### DomainDependency

Cross-domain data dependencies (beyond event wiring). Shown as dashed arrows at L2.

| Field | Type | Description |
|-------|------|-------------|
| `domain` | string | Domain ID this domain depends on |
| `reason` | string | Why the dependency exists |
| `flows_affected` | string[]? | Which flows in this domain use the dependency |

```yaml
depends_on:
  - domain: settings
    reason: "Reads API keys and social account preferences"
    flows_affected: [generate-drafts, check-social-sources]
  - domain: subjects
    reason: "Reads subject keywords for content scoring"
    flows_affected: [analyze-content]
```

---

## 4. Supplementary Spec Files

These files provide context for code generation and project configuration. Most are read by `/ddd-implement` and other slash commands. Some (like `system.yaml` zones and data flows) are also used by the DDD Tool for L1/L2 visualization. Create them at the root of `specs/`.

### 4.1 system.yaml

**Path:** `specs/system.yaml`

Project identity and tech stack choices:

```yaml
name: my-project
version: "1.0.0"
description: Brief description of the project

tech_stack:
  language: TypeScript
  runtime: Node.js 20
  framework: Express 4
  database: PostgreSQL 16
  orm: Prisma
  cache: Redis 7
  queue: BullMQ
  auth: JWT + bcrypt
  # queue field determines event infrastructure (see note below)

environments:
  - name: development
    url: http://localhost:3000
  - name: staging
    url: https://staging.example.com
  - name: production
    url: https://api.example.com

zones:
  - id: discovery-loop
    name: Discovery Loop
    domains: [discovery]
  - id: shared-pipeline
    name: Shared Pipeline
    domains: [content, publishing]

integrations:
  twitter_api:
    base_url: "https://api.twitter.com/2"
    auth:
      method: oauth2
      credentials_env: TWITTER_BEARER_TOKEN
    rate_limits:
      requests_per_window: 300
      window_seconds: 900
    retry:
      max_attempts: 3
      backoff_ms: 1000
    headers:
      User-Agent: "my-app/1.0"
    used_by_domains: [discovery, monitoring, publishing]

  stripe:
    base_url: "https://api.stripe.com/v1"
    auth:
      method: api_key
      credentials_env: STRIPE_SECRET_KEY
    rate_limits:
      requests_per_second: 25
    timeout_ms: 30000
```

> **Note:** The `queue` field determines event infrastructure: when set (e.g., `BullMQ`), `/ddd-scaffold` generates queue-based event infrastructure and `/ddd-implement` uses it for async events. When absent, use an in-process EventEmitter for domain events.

#### IntegrationConfig

| Field | Type | Description |
|-------|------|-------------|
| `base_url` | string | Base URL for the external API |
| `auth` | object | `{ method: 'api_key' \| 'oauth2' \| 'bearer', credentials_env: string }` |
| `rate_limits` | object? | `{ requests_per_second?, requests_per_window?, window_seconds? }` |
| `retry` | object? | `{ max_attempts?, backoff_ms?, strategy?: 'fixed' \| 'linear' \| 'exponential', jitter?: boolean }` |
| `timeout_ms` | number? | Request timeout in milliseconds |
| `headers` | `Record<string, string>?` | Default headers for all requests |
| `used_by_domains` | string[]? | Domains that use this integration (shown at L1) |

#### SystemZone

| Field | Type | Description |
|-------|------|-------------|
| `id` | string | Zone ID |
| `name` | string | Display name |
| `domains` | string[] | Domain IDs in this zone |

> **Note:** `service_call` nodes can reference integrations by name — `/ddd-implement` uses the integration config to set base URL, auth headers, retry, and rate limiting instead of repeating them per node.

#### DataFlow

Inter-zone directed data flow arrows, shown at L1 to visualize high-level architectural data movement.

| Field | Type | Description |
|-------|------|-------------|
| `from` | string | Source zone or domain ID |
| `to` | string | Target zone or domain ID |
| `label` | string | Description of what flows |
| `volume` | string? | `'low' \| 'medium' \| 'high'` — visual thickness hint |
| `style` | string? | `'solid' \| 'dashed'` — visual distinction (default: solid) |

```yaml
data_flows:
  - from: discovery-loop
    to: shared-pipeline
    label: "ContentDiscovered events"
    volume: high
  - from: shared-pipeline
    to: publishing-loop
    label: "DraftApproved events"
    volume: medium
  - from: discovery-loop
    to: publishing-loop
    label: "Source suggestions (flywheel)"
    volume: low
    style: dashed
```

#### Schedule

Scheduling topology — surfaces cron schedules at L1 so you can see when flows run.

| Field | Type | Description |
|-------|------|-------------|
| `frequency` | string | Cron expression |
| `label` | string | Human-readable schedule description |
| `flows` | string[] | Flow references in `domain/flow-id` format |

```yaml
schedules:
  - frequency: "*/15 * * * *"
    label: "Every 15 minutes"
    flows: [monitoring/check-social-sources]
  - frequency: "0 */4 * * *"
    label: "Every 4 hours"
    flows: [discovery/keyword-search]
  - frequency: "0 2 * * *"
    label: "Daily at 2am"
    flows: [discovery/suggest-source, monitoring/full-source-check]
```

#### Characteristics

System-level badges shown at L1 — high-level properties of the system.

```yaml
characteristics:
  - "Event-driven"
  - "6 cron schedules"
  - "6 external APIs"
  - "2-loop flywheel"
```

#### Pipeline

Cross-domain event chains that trace a complete pipeline across domains. Shown at L2 as a pipeline view.

| Field | Type | Description |
|-------|------|-------------|
| `id` | string | Pipeline ID |
| `name` | string | Display name |
| `description` | string? | What this pipeline does end-to-end |
| `steps` | PipelineStep[] | Ordered steps across domains |
| `spans_domains` | string[] | All domains involved |

**PipelineStep fields:**

| Field | Type | Description |
|-------|------|-------------|
| `domain` | string | Domain ID |
| `flow` | string | Flow ID within the domain |
| `event_out` | string? | Event emitted to trigger the next step |

```yaml
pipelines:
  - id: content-pipeline
    name: Content Processing Pipeline
    description: "End-to-end from discovery to publication"
    steps:
      - { domain: monitoring, flow: check-social-sources, event_out: NewContentDiscovered }
      - { domain: content, flow: analyze-content, event_out: DraftGenerated }
      - { domain: approval, flow: review-draft, event_out: DraftApproved }
      - { domain: publishing, flow: publish-to-twitter, event_out: PostPublished }
    spans_domains: [monitoring, content, approval, publishing]
```

### 4.2 architecture.yaml

**Path:** `specs/architecture.yaml`

Project conventions, infrastructure, and API design standards:

```yaml
project_structure:
  src/
    routes/: Express route handlers
    services/: Business logic (one service file per flow)
    repositories/: Data access layer (shared across flows)
    models/: Prisma models
    middleware/: Auth, validation, error handling
    utils/: Shared utilities
    types/: TypeScript type definitions
  tests/
    unit/: Unit tests
    integration/: Integration tests

naming_conventions:
  files: kebab-case
  classes: PascalCase
  functions: camelCase
  variables: camelCase
  constants: UPPER_SNAKE_CASE
  database_tables: snake_case
  api_endpoints: kebab-case

dependencies:
  runtime:
    - express: "^4.18"
    - prisma: "^5.0"
    - zod: "^3.22"
    - jsonwebtoken: "^9.0"
  dev:
    - vitest: "^1.0"
    - supertest: "^6.3"
    - typescript: "^5.3"

infrastructure:
  containerization: Docker
  orchestration: Kubernetes
  ci_cd: GitHub Actions
  monitoring: Prometheus + Grafana
  logging: Pino structured JSON

api_design:
  versioning: URL prefix (/api/v1)
  pagination:
    style: cursor
    default_limit: 20
    max_limit: 100
    response_format:
      data: "array of items"
      cursor: "next page cursor"
      has_more: "boolean"
  filtering:
    style: query parameters
    operators: "eq, ne, gt, gte, lt, lte, in, like"
    example: "?status=active&created_at[gte]=2025-01-01"
  error_format:
    error:
      code: "ERROR_CODE"
      message: "Human-readable message"
      details: "Optional additional context"

testing:
  framework: vitest
  runner: "npx vitest run"
  coverage_target: 80%
  patterns:
    unit: "tests/unit/**/*.test.ts"
    integration: "tests/integration/**/*.test.ts"

deployment:
  strategy: rolling update
  health_check: /health
  readiness_check: /ready

cross_cutting_patterns:
  # Populated by /ddd-reflect and /ddd-promote as implementation wisdom is captured.
  # Each pattern defines a project-wide convention applied by /ddd-implement.
  # See Section 12.3 for full schema and examples.
  # Example:
  #   soft_delete:
  #     description: "All reads exclude soft-deleted records"
  #     used_by_domains: [users, orders, billing]
  #     convention: "Add deletedAt: null to all data_store read filters"
```

### 4.3 config.yaml

**Path:** `specs/config.yaml`

Environment variable schema — defines what env vars the project needs:

```yaml
required:
  - name: DATABASE_URL
    type: string
    description: PostgreSQL connection string
    example: "postgresql://user:pass@localhost:5432/mydb"
    sensitive: true

  - name: JWT_SECRET
    type: string
    description: Secret key for JWT signing
    sensitive: true

  - name: REDIS_URL
    type: string
    description: Redis connection string
    example: "redis://localhost:6379"

optional:
  - name: PORT
    type: number
    default: 3000
    description: Server port

  - name: LOG_LEVEL
    type: string
    default: "info"
    description: Logging level (debug, info, warn, error)

  - name: CORS_ORIGINS
    type: string
    default: "*"
    description: Comma-separated allowed origins

  - name: RATE_LIMIT_RPM
    type: number
    default: 60
    description: Requests per minute per IP
```

> **Note:** The `templates/config-template.yaml` uses a map-of-objects format (`DATABASE_URL: { type, format, ... }`) with additional fields like `format`, `min`, `max`, `enum`. Both formats are accepted — use whichever fits your project. `/ddd-create` generates the array format shown above.

### 4.4 errors.yaml

**Path:** `specs/shared/errors.yaml`

Structured error codes with HTTP status mappings:

```yaml
errors:
  VALIDATION_ERROR:
    http_status: 422
    message_template: "Validation failed: {details}"
    log_level: warn

  UNAUTHORIZED:
    http_status: 401
    message_template: "Authentication required"
    log_level: warn

  FORBIDDEN:
    http_status: 403
    message_template: "Insufficient permissions"
    log_level: warn

  NOT_FOUND:
    http_status: 404
    message_template: "{resource} not found"
    log_level: info

  DUPLICATE_ENTRY:
    http_status: 409
    message_template: "{resource} already exists"
    log_level: warn

  RATE_LIMITED:
    http_status: 429
    message_template: "Too many requests, try again in {retry_after}s"
    log_level: warn

  INTERNAL_ERROR:
    http_status: 500
    message_template: "An unexpected error occurred"
    log_level: error

  SERVICE_UNAVAILABLE:
    http_status: 503
    message_template: "{service} is temporarily unavailable"
    log_level: error
```

### 4.5 Schema Definitions

**Path:** `specs/schemas/{model}.yaml`

Data model definitions used across flows. Start with a base model:

**`specs/schemas/_base.yaml`** — fields inherited by all models:

```yaml
name: Base
description: Common fields for all models
fields:
  - name: id
    type: uuid
    required: true
    description: Primary key (auto-generated)
  - name: created_at
    type: datetime
    required: true
    description: Creation timestamp (auto-set)
  - name: updated_at
    type: datetime
    required: true
    description: Last update timestamp (auto-set)
  - name: deleted_at
    type: datetime
    required: false
    description: Soft delete timestamp (null = active)
```

**`specs/schemas/user.yaml`** — example model:

```yaml
name: User
description: User account
inherits: _base
fields:
  - name: email
    type: string
    required: true
    format: email
    constraints:
      unique: true
      max_length: 255
  - name: password_hash
    type: string
    required: true
    encrypted: true
    constraints:
      max_length: 255
  - name: name
    type: string
    required: true
    constraints:
      max_length: 100
  - name: role
    type: enum
    required: true
    values: [user, admin]
    default: user
  - name: is_verified
    type: boolean
    required: true
    default: false
  - name: last_login_at
    type: datetime
    required: false

indexes:
  - fields: [email]
    unique: true
  - fields: [role, created_at]

relationships:
  - name: orders
    type: has_many
    target: Order
    foreign_key: user_id

transitions:
  field: role
  states:
    - from: user
      to: [admin]
    - from: admin
      to: [user]
  on_invalid: reject
```

**Schema field options** — each field in a schema's `fields` array supports these properties:

| Field | Type | Description |
|-------|------|-------------|
| `name` | string | Field name |
| `type` | string | Data type (string, number, boolean, uuid, datetime, enum, json, decimal) |
| `required` | boolean? | Whether the field is required |
| `format` | string? | Format hint (e.g., "email") |
| `default` | any? | Default value |
| `values` | string[]? | Allowed values (for enum type) |
| `ref` | string? | Reference to a shared enum in `shared/types.yaml` (e.g., `platform`). Use instead of duplicating `values` |
| `constraints` | object? | `{ unique?, max_length?, min?, max? }` |
| `encrypted` | boolean? | Field value is encrypted at rest |

The optional `transitions:` section defines valid state machine transitions for lifecycle fields:

```yaml
# Example: Order status lifecycle
transitions:
  field: status
  states:
    - from: pending
      to: [confirmed, cancelled]
    - from: confirmed
      to: [shipped, cancelled]
    - from: shipped
      to: [delivered]
  on_invalid: reject   # reject | warn | log
```

#### Transitions

| Field | Type | Description |
|-------|------|-------------|
| `field` | string | The enum field this state machine applies to |
| `states` | `Array<{ from: string, to: string[] }>` | Valid transitions |
| `on_invalid` | `'reject' \| 'warn' \| 'log'` | What happens on invalid transition attempt |

> **Note:** When a schema has `transitions`, `/ddd-implement` generates validation logic that enforces valid state changes and rejects (or warns/logs) invalid ones.

#### Indexes

Database indexes for query performance. These are design decisions — "this data is always queried by X" — not implementation details.

| Field | Type | Description |
|-------|------|-------------|
| `fields` | string[] | Column names (composite index if multiple) |
| `unique` | boolean? | Unique constraint (default: false) |
| `type` | string? | Index type (`'btree'` \| `'hash'` \| `'gin'` \| `'gist'`). Default: btree |
| `description` | string? | Why this index exists |

```yaml
indexes:
  - fields: [email]
    unique: true
  - fields: [category, user_id]
    description: "Inbox items filtered by category per user"
  - fields: [source_id]
    unique: true
    description: "Prevent duplicate imports from same source"
  - fields: [tags]
    type: gin
    description: "Array contains queries on tags"
```

> **Note:** `/ddd-scaffold` reads indexes to generate database migration definitions. `/ddd-implement` reads them to understand available query patterns.

#### Seed

Initial data that must exist for the system to function. Enums with fixed values, default records, reference data.

| Field | Type | Description |
|-------|------|-------------|
| `name` | string | Seed set name |
| `description` | string? | What this data is and why it's needed |
| `strategy` | `'migration' \| 'fixture' \| 'script'` | How to apply — migration (runs once, part of DB setup), fixture (test/dev data, re-runnable), script (custom logic) |
| `data` | array? | Inline seed records (for small, fixed datasets) |
| `source` | string? | Reference to external data source (for large imports) |
| `count_estimate` | number? | Approximate record count (for imports without inline data) |

```yaml
seed:
  - name: gtd-categories
    description: "9 GTD categories — system-defined, immutable"
    strategy: migration
    data:
      - { id: inbox, label: Inbox, sort_order: 1 }
      - { id: projects, label: Projects, sort_order: 2 }
      - { id: next-actions, label: "Next Actions", sort_order: 3 }
      - { id: waiting-for, label: "Waiting For", sort_order: 4 }
      - { id: someday-maybe, label: "Someday/Maybe", sort_order: 5 }
      - { id: reference, label: Reference, sort_order: 6 }
      - { id: review, label: Review, sort_order: 7 }
      - { id: ideas, label: Ideas, sort_order: 8 }
      - { id: archive, label: Archive, sort_order: 9 }

  - name: initial-apple-notes-import
    description: "Existing Apple Notes from user's GTD folder structure"
    strategy: script
    source: "Apple Notes via osascript sync"
    count_estimate: 200
```

> **Note:** `/ddd-scaffold` reads seed data with `strategy: migration` to generate initial database seed files. Seed data with `strategy: fixture` is generated as test helper data. Seed data with `strategy: script` is documented but requires custom implementation.

### 4.6 shared/types.yaml

**Path:** `specs/shared/types.yaml`

Shared enums and value objects used across multiple schemas:

```yaml
# specs/shared/types.yaml
enums:
  platform:
    values: [twitter, linkedin, medium, rss, web]
    description: Supported content platforms

  content_status:
    values: [pending, analyzed, relevant, irrelevant, error]
    description: Content processing lifecycle

value_objects:
  money:
    fields:
      - name: amount
        type: decimal
      - name: currency
        type: string
    description: Monetary amount with currency
```

#### Enum Definition

| Field | Type | Description |
|-------|------|-------------|
| `values` | string[] | Allowed values |
| `description` | string? | What this enum represents |

#### Value Object Definition

| Field | Type | Description |
|-------|------|-------------|
| `fields` | `Array<{ name, type }>` | Component fields |
| `description` | string? | What this value object represents |

> **Note:** Reference shared enums in schemas with `type: enum, ref: platform` instead of duplicating `values:` arrays. `/ddd-implement` reads this file to generate shared type definitions.

### 4.7 UI Specs

**Path:** `specs/ui/pages.yaml` (page registry) + `specs/ui/{page-id}.yaml` (per-page specs)

UI specs define the interface pillar — pages, components, navigation, data bindings, forms, and visual states. They are declarative (component trees with data bindings), not flow-based. Each page spec references backend flow specs via `data_source` fields, linking the interface and logic pillars.

#### pages.yaml — Page Registry

The central registry of all pages, their routes, and navigation structure:

```yaml
# specs/ui/pages.yaml
app_type: web                        # web | mobile | desktop | cli
framework: Next.js 14
router: app                          # app (Next.js app router) | pages | hash | native
state_management: zustand             # zustand | redux | context | jotai | none
component_library: shadcn/ui          # shadcn/ui | mui | chakra | ant | radix | custom | none

theme:
  color_scheme: light                 # light | dark | system
  primary_color: "#2563eb"
  font_family: "Inter"
  border_radius: 8

pages:
  - id: dashboard
    route: /
    name: Dashboard
    description: "Main view — inbox count, today's actions, project health"
    layout: sidebar
  - id: inbox
    route: /inbox
    name: Inbox Processing
    description: "Process inbox items one-at-a-time with AI suggestions"
    layout: centered
  - id: settings
    route: /settings
    name: Settings
    description: "Connector config, TELOS editing, taxonomy management"
    layout: sidebar

navigation:
  type: sidebar                       # sidebar | topbar | tabs | drawer | none
  items:
    - { page: dashboard, icon: home, label: Dashboard }
    - { page: inbox, icon: inbox, label: Inbox, badge: inbox_count }
    - { page: settings, icon: settings, label: Settings }

shared_components:
  - id: item-card
    description: "Displays a GTD item with title, source, category badge, and actions"
    used_by: [dashboard, inbox]
  - id: ai-suggestion
    description: "Shows AI category suggestion with confidence, reasoning, accept/reject buttons"
    used_by: [inbox]
```

#### PagesConfig Fields

| Field | Type | Description |
|-------|------|-------------|
| `app_type` | `'web' \| 'mobile' \| 'desktop' \| 'cli'` | Application type — determines which component patterns apply |
| `framework` | string | Frontend framework (e.g., "Next.js 14", "React 18", "React Native") |
| `router` | string? | Router type (e.g., "app", "pages", "hash", "native") |
| `state_management` | string? | Client-side state approach (e.g., "zustand", "redux", "context") |
| `component_library` | string? | UI component library (e.g., "shadcn/ui", "mui", "chakra") |
| `theme` | ThemeConfig? | Global theme settings |
| `pages` | PageEntry[] | All pages in the application |
| `navigation` | NavigationConfig? | Navigation structure |
| `shared_components` | SharedComponent[]? | Reusable components used across multiple pages |

#### ThemeConfig

| Field | Type | Description |
|-------|------|-------------|
| `color_scheme` | `'light' \| 'dark' \| 'system'` | Default color scheme |
| `primary_color` | string? | Primary brand color (hex) |
| `font_family` | string? | Primary font |
| `border_radius` | number? | Default border radius in px |

#### PageEntry

| Field | Type | Description |
|-------|------|-------------|
| `id` | string | Page ID (kebab-case) — used as filename for per-page spec |
| `route` | string | URL route (e.g., "/", "/inbox", "/settings/:tab") |
| `name` | string | Display name |
| `description` | string? | What this page does |
| `layout` | `'sidebar' \| 'full' \| 'centered' \| 'split' \| 'stacked'` | Page layout style |
| `auth_required` | boolean? | Whether this page requires authentication (default: false) |

#### NavigationConfig

| Field | Type | Description |
|-------|------|-------------|
| `type` | `'sidebar' \| 'topbar' \| 'tabs' \| 'drawer' \| 'none'` | Navigation style |
| `items` | NavigationItem[] | Navigation entries |

#### NavigationItem

| Field | Type | Description |
|-------|------|-------------|
| `page` | string | Page ID reference |
| `icon` | string? | Icon name (framework-specific, e.g., "home", "inbox") |
| `label` | string | Display label |
| `badge` | string? | Dynamic badge value — references a data field (e.g., "inbox_count") |

#### SharedComponent

| Field | Type | Description |
|-------|------|-------------|
| `id` | string | Component ID (kebab-case) |
| `description` | string | What this component renders |
| `used_by` | string[] | Page IDs that use this component |
| `props` | ComponentProp[]? | Component props interface |

> **Extraction rule:** If the same UI pattern (e.g., a notification badge, entity card, AI suggestion panel) appears in 2+ pages, extract it as a shared component in `pages.yaml` → `shared_components` and reference by ID in page specs.

#### Per-Page Spec — {page-id}.yaml

Each page gets its own spec file with detailed sections, components, data bindings, and states:

```yaml
# specs/ui/dashboard.yaml
page: dashboard
route: /

sections:
  - id: inbox-summary
    component: stat-card
    position: top-left
    label: "Inbox"
    data_source: dashboard/get-dashboard-data     # references backend flow
    fields:
      value: "$.inbox_count"
      subtitle: "items to process"
      urgency:
        field: "$.inbox_count"
        rules:
          - { threshold: 50, level: critical, color: red }
          - { threshold: 20, level: warning, color: amber }
          - { threshold: 0, level: normal, color: green }
    actions:
      click: { navigate: /inbox }

  - id: todays-actions
    component: item-list
    position: main
    label: "Today's Next Actions"
    data_source: gtd-engine/list-items
    query:
      category: next-actions
      context: "$.current_context"
      sort: priority
    empty_state:
      message: "No actions for today"
      icon: check-circle
    item_template:
      title: "$.title"
      subtitle: "$.context · $.time_estimate"
      badge: "$.priority"
    item_actions:
      - label: Done
        icon: check
        flow: gtd-engine/categorize-item
        args: { category: archive }
        confirm: false
      - label: Defer
        icon: clock
        flow: gtd-engine/update-item
        args: { due_date: tomorrow }

  - id: projects-health
    component: card-grid
    position: main
    label: "Projects"
    data_source: dashboard/get-dashboard-data
    fields:
      items: "$.projects"
    item_template:
      title: "$.name"
      subtitle: "$.next_action_count actions"
      status:
        field: "$.has_next_action"
        true: { label: Active, color: green }
        false: { label: Stuck, color: red }

  - id: sync-status
    component: status-bar
    position: footer
    data_source: dashboard/realtime-updates       # WebSocket
    fields:
      items: "$.connector_statuses"
    item_template:
      label: "$.connector_name"
      status: "$.status"
      last_sync: "$.last_sync_at"

forms: []                                          # No forms on dashboard

state:
  store: dashboard-store                           # references domain.yaml stores
  initial_fetch: [dashboard/get-dashboard-data]    # API calls on page load
  realtime: dashboard/realtime-updates             # WebSocket subscription

loading: skeleton                                  # skeleton | spinner | blur
error: retry-banner                                # retry-banner | error-page | toast
refresh: pull-to-refresh                           # pull-to-refresh | auto-30s | manual | none
```

Example of a page with forms:

```yaml
# specs/ui/inbox.yaml
page: inbox
route: /inbox

sections:
  - id: inbox-header
    component: page-header
    position: top
    label: "Inbox"
    data_source: inbox/get-inbox-stats
    fields:
      count: "$.total"
      subtitle: "$.total items · $.ai_suggested auto-suggested"
    actions:
      - label: Auto-process
        icon: zap
        flow: inbox/process-inbox-item
        args: { mode: batch }
        confirm: true
        confirm_message: "Auto-process all items above confidence threshold?"

  - id: current-item
    component: detail-card
    position: main
    label: "Current Item"
    data_source: inbox/process-inbox-item
    fields:
      title: "$.item.title"
      body: "$.item.body"
      source: "$.item.source.type"
      captured_at: "$.item.source.captured_at"
      ai_suggestion:
        category: "$.item.ai.suggested_category"
        confidence: "$.item.ai.confidence"
        reasoning: "$.item.ai.reasoning"

  - id: categorize-actions
    component: button-group
    position: bottom
    label: "Categorize"
    buttons:
      - label: "Accept AI"
        icon: check
        variant: primary
        flow: inbox/accept-suggestion
        visible_when: "$.item.ai.confidence > 0"
      - label: Projects
        flow: inbox/override-suggestion
        args: { category: projects }
      - label: "Next Actions"
        flow: inbox/override-suggestion
        args: { category: next-actions }
      - label: "Waiting For"
        flow: inbox/override-suggestion
        args: { category: waiting-for }
      - label: Reference
        flow: inbox/override-suggestion
        args: { category: reference }
      - label: "Someday/Maybe"
        flow: inbox/override-suggestion
        args: { category: someday-maybe }
      - label: Archive
        flow: inbox/override-suggestion
        args: { category: archive }
        variant: ghost

forms:
  - id: enrich-item
    label: "Enrich"
    description: "Add context, energy, time estimate, project link"
    position: sidebar
    fields:
      - name: contexts
        type: multi-select
        label: "Context"
        options_source: shared/types.yaml#context
        placeholder: "Select contexts..."
      - name: energy
        type: select
        label: "Energy"
        options: [low, medium, high]
      - name: time_estimate
        type: select
        label: "Time"
        options: [5min, 15min, 30min, 1h, "2h+"]
      - name: project_id
        type: search-select
        label: "Project"
        search_source: gtd-engine/list-items
        search_args: { category: projects }
        display_field: title
        value_field: id
        allow_empty: true
      - name: tags
        type: tag-input
        label: "Tags"
        autocomplete_source: gtd-engine/list-items
        autocomplete_field: tags
      - name: due_date
        type: date
        label: "Due Date"
        allow_empty: true
    submit:
      flow: gtd-engine/update-item
      label: "Save"

state:
  store: inbox-store
  initial_fetch: [inbox/get-inbox-stats, inbox/process-inbox-item]
  realtime: null

loading: skeleton
error: retry-banner
refresh: manual
```

#### PageSpec Fields

| Field | Type | Description |
|-------|------|-------------|
| `page` | string | Page ID (must match a `pages` entry in pages.yaml) |
| `route` | string | URL route (matches pages.yaml) |
| `sections` | PageSection[] | Visual sections of the page |
| `forms` | FormSpec[]? | Forms on this page |
| `state` | PageState? | Client-side state configuration |
| `loading` | `'skeleton' \| 'spinner' \| 'blur'` | Loading state style |
| `error` | `'retry-banner' \| 'error-page' \| 'toast'` | Error state style |
| `refresh` | `'pull-to-refresh' \| 'auto-30s' \| 'manual' \| 'none'`? | Data refresh strategy |

#### PageSection

| Field | Type | Description |
|-------|------|-------------|
| `id` | string | Section ID (unique within page) |
| `component` | string | Component type (e.g., "stat-card", "item-list", "card-grid", "detail-card", "button-group", "page-header", "status-bar") or shared component ID |
| `position` | string? | Layout position (e.g., "top", "main", "sidebar", "footer", "top-left") |
| `label` | string | Section heading |
| `data_source` | string? | Backend flow reference in `domain/flow-id` format |
| `query` | `Record<string, unknown>`? | Query parameters passed to the data source |
| `fields` | `Record<string, unknown>`? | Data field mappings using `$.field` syntax |
| `item_template` | `Record<string, unknown>`? | Template for rendering list/grid items |
| `empty_state` | EmptyState? | What to show when data is empty |
| `actions` | `Record<string, unknown>`? | Section-level actions (click handlers, navigation) |
| `item_actions` | ItemAction[]? | Per-item action buttons |
| `buttons` | ButtonSpec[]? | Button group entries (for button-group component) |
| `visible_when` | string? | Conditional visibility expression |

#### Component Type Field Reference

Different component types use different subsets of PageSection fields. Here's what each expects:

| Component | Key Fields | Notes |
|-----------|------------|-------|
| `stat-card` | `fields.value`, `fields.subtitle`, `fields.urgency` (with `rules`), `actions` (e.g., `click: {navigate}`) | Single metric display |
| `item-list` | `data_source`, `item_template` (title/subtitle/badge/timestamp), `item_actions`, `empty_state`, optional `pagination`/`virtual_scroll` | Scrollable list |
| `card-grid` | `data_source`, `fields.columns` (responsive: desktop/tablet/mobile), `item_template` (image/title/description/footer), `item_actions`, `empty_state` | Responsive grid |
| `detail-card` | `data_source`, `fields` mapping with `$.field` syntax, `actions` (edit/delete/archive), optional `tabs` | Single record view |
| `button-group` | `buttons` (ButtonSpec[]) with `label`, `flow`, `args`, `variant`, `icon`, `visible_when`, `confirm` | Action buttons |
| `page-header` | `fields` (title, subtitle), `breadcrumbs`, `actions` (button-group for page-level actions) | Page title area |
| `status-bar` | `fields.items` array with `label`, `value` ($.field), `color_when` conditions | Status indicators |
| `chart` | `data_source`, `fields` (series, labels, values), `chart_type` (line/bar/pie/area/donut) | Data visualization |
| `filter-bar` | `fields` (FormField[] for filter inputs), binds to sibling section's `query` | Inline filters for lists/grids |

> To use a shared component, set `component` to the shared component's ID from `pages.yaml` → `shared_components`.

#### FormSpec

| Field | Type | Description |
|-------|------|-------------|
| `id` | string | Form ID |
| `label` | string | Form heading |
| `description` | string? | Help text shown above the form |
| `position` | string? | Layout position |
| `fields` | FormField[] | Form fields |
| `submit` | FormSubmit | Submit configuration |

#### FormField

| Field | Type | Description |
|-------|------|-------------|
| `name` | string | Field name (sent to backend) |
| `type` | string | Input type: `'text' \| 'number' \| 'select' \| 'multi-select' \| 'search-select' \| 'date' \| 'datetime' \| 'date-range' \| 'textarea' \| 'toggle' \| 'tag-input' \| 'file' \| 'color' \| 'slider'` |
| `label` | string | Display label |
| `placeholder` | string? | Placeholder text |
| `required` | boolean? | Whether field is required (default: false) |
| `options` | string[]? | Static options (for select/multi-select) |
| `options_source` | string? | Dynamic options from spec reference (e.g., `shared/types.yaml#context`) |
| `search_source` | string? | Backend flow for search-select autocomplete |
| `search_args` | `Record<string, unknown>`? | Arguments passed to search source |
| `autocomplete_source` | string? | Backend flow for tag autocomplete |
| `autocomplete_field` | string? | Field to extract from autocomplete results |
| `display_field` | string? | Field to display in dropdowns |
| `value_field` | string? | Field to use as value in dropdowns |
| `allow_empty` | boolean? | Whether empty/null is valid |
| `default` | any? | Default value |
| `validation` | string? | Validation rule description |
| `visible_when` | string? | Conditional visibility expression |

#### FormSubmit

| Field | Type | Description |
|-------|------|-------------|
| `flow` | string | Backend flow to call on submit (`domain/flow-id`) |
| `args` | `Record<string, unknown>`? | Additional arguments merged with form data |
| `label` | string | Submit button label |
| `loading_label` | string? | Text shown on button during submission |
| `variant` | string? | Button variant (e.g., "primary", "secondary") |
| `success` | `{ message: string, redirect?: string, action?: string }`? | Success behavior — message to show, page to redirect to, and/or UI action to trigger |
| `error` | `{ message: string, retry?: boolean }`? | Error behavior — message to show and whether to offer retry |

#### ItemAction

| Field | Type | Description |
|-------|------|-------------|
| `label` | string | Button label |
| `icon` | string? | Icon name |
| `flow` | string | Backend flow to call (`domain/flow-id`) |
| `args` | `Record<string, unknown>`? | Arguments passed to the flow |
| `confirm` | boolean? | Show confirmation dialog (default: false) |
| `confirm_message` | string? | Custom confirmation message |
| `variant` | string? | Button variant |
| `visible_when` | string? | Conditional visibility expression |

#### ButtonSpec

Same fields as ItemAction, used in `button-group` component sections.

#### EmptyState

| Field | Type | Description |
|-------|------|-------------|
| `message` | string | Message to display |
| `icon` | string? | Icon name |
| `action` | object? | Optional CTA button `{ label, navigate?, flow? }` |

#### PageState

| Field | Type | Description |
|-------|------|-------------|
| `store` | string? | Client-side store name (references `stores` in domain.yaml) |
| `initial_fetch` | string[]? | Backend flows to call on page load |
| `realtime` | string? | WebSocket/SSE flow for real-time updates (null = no realtime) |

> **Note:** UI specs are declarative — they describe what the page shows and how it binds to data, not how to render it. `/ddd-implement` reads these specs alongside the backend flow specs to generate page components with proper data fetching, state management, form handling, and loading/error states. `/ddd-scaffold` creates the page file structure from `pages.yaml`.

### 4.8 infrastructure.yaml

**Path:** `specs/infrastructure.yaml`

Defines the services needed to run the project, how they connect, and how to start them. Kept slim — focused on what's needed to run and connect services, not a full DevOps specification.

```yaml
# specs/infrastructure.yaml
services:
  - id: backend
    type: server
    runtime: "Node.js 20"
    framework: "Express 4"
    entry: src/server/index.ts
    port: 3001
    health: /api/v1/health
    depends_on: [database, cache]
    dev_command: "npx tsx watch src/server/index.ts"

  - id: frontend
    type: server
    runtime: "Next.js 14"
    entry: src/app/
    port: 3000
    depends_on: [backend]
    dev_command: "npx next dev"

  - id: database
    type: datastore
    engine: "PostgreSQL 16"
    port: 5432
    setup: "npx prisma db push"

  - id: cache
    type: datastore
    engine: "Redis 7"
    port: 6379

startup_order: [database, cache, backend, frontend]

deployment:
  local:
    strategy: process-manager
  production:
    strategy: docker-compose
```

**Design principles:**
- Every service mentioned in `system.yaml` tech stack MUST appear in `infrastructure.yaml` — no implicit services
- Ports must not conflict — assign unique ports to each service
- `depends_on` must form a DAG (no circular dependencies)
- `startup_order` must list every service ID exactly once

#### ServiceConfig

| Field | Type | Description |
|-------|------|-------------|
| `id` | string | Service identifier |
| `type` | `'server' \| 'datastore' \| 'worker' \| 'proxy'` | Service type |
| `runtime` | string? | Runtime/engine (e.g., "Node.js 20", "PostgreSQL 16") |
| `framework` | string? | Framework (e.g., "Express 4", "Next.js 14") |
| `entry` | string? | Entry point file or directory |
| `port` | number? | Port number |
| `health` | string? | Health check endpoint path |
| `depends_on` | string[]? | Service IDs this service requires |
| `dev_command` | string? | Command to start in development |
| `setup` | string? | One-time setup command (e.g., DB migration) |
| `engine` | string? | Engine name for datastores |

#### DeploymentConfig

| Field | Type | Description |
|-------|------|-------------|
| `local` | object | `{ strategy: 'process-manager' \| 'docker-compose' }` |
| `production` | object? | `{ strategy: 'docker-compose' \| 'kubernetes' \| 'serverless', reverse_proxy?: string, ssl?: string }` |

> **Note:** `/ddd-scaffold` reads `infrastructure.yaml` to generate startup scripts, `docker-compose.yaml` (if production strategy is docker-compose), and `package.json` scripts. `/ddd-implement` reads it to configure API client base URLs and WebSocket endpoints in frontend code.

---

## 5. Flow YAML

**Path:** `specs/domains/{domain-id}/flows/{flow-id}.yaml`

This is the core spec file. It defines the entire flow graph.

```yaml
flow:
  id: user-register
  name: User Registration
  type: traditional
  domain: users
  description: Register a new user account

trigger:
  id: trigger-abc123
  type: trigger
  position: { x: 250, y: 50 }
  connections:
    - targetNodeId: input-def456
  spec:
    event: HTTP POST /auth/register
    source: API Gateway
    description: Registration request received
  label: API Request

nodes:
  - id: input-def456
    type: input
    position: { x: 250, y: 170 }
    connections:
      - targetNodeId: decision-jkl012
        sourceHandle: "valid"
      - targetNodeId: terminal-validation-error
        sourceHandle: "invalid"
    spec:
      fields:
        - name: email
          type: string
          required: true
        - name: password
          type: string
          required: true
        - name: name
          type: string
          required: true
      validation: "Email format, password min 8 chars"
      description: Registration form data
    label: Registration Form

  - id: decision-jkl012
    type: decision
    position: { x: 250, y: 300 }
    connections:
      - targetNodeId: process-ghi789
        sourceHandle: "true"
      - targetNodeId: terminal-error
        sourceHandle: "false"
    spec:
      condition: email.is_available?
      trueLabel: "Available"
      falseLabel: "Already taken"
      description: Check if email already registered
    label: Email Available?

  - id: process-ghi789
    type: process
    position: { x: 250, y: 430 }
    connections:
      - targetNodeId: terminal-success
    spec:
      action: Hash password and create user record
      service: UserService.register
      description: Create user in database
    label: Create User

  - id: terminal-success
    type: terminal
    position: { x: 150, y: 560 }
    connections: []
    spec:
      outcome: success
      description: Return user object with JWT token
      status: 201
      body:
        message: "User registered successfully"
        user: "$.user"
        token: "$.jwt_token"
    label: Success

  - id: terminal-error
    type: terminal
    position: { x: 400, y: 560 }
    connections: []
    spec:
      outcome: error
      description: Return 409 Conflict
      status: 409
      body:
        error: "DUPLICATE_ENTRY"
        message: "Email already registered"
    label: Duplicate Email

  - id: terminal-validation-error
    type: terminal
    position: { x: 450, y: 300 }
    connections: []
    spec:
      outcome: error
      description: Return 422 Validation Error
      status: 422
      body:
        error: "VALIDATION_ERROR"
        message: "$.validation_errors"
    label: Invalid Input

metadata:
  created: "2025-01-10T10:00:00Z"
  modified: "2025-01-15T14:30:00Z"
```

### FlowDocument Structure

| Field | Type | Description |
|-------|------|-------------|
| `flow` | object | Flow metadata |
| `flow.id` | string | Unique flow ID |
| `flow.name` | string | Display name |
| `flow.type` | `'traditional' \| 'agent'` | Flow category. Flows can be further categorized via `tags` on the DomainFlowEntry |
| `flow.domain` | string | Parent domain ID |
| `flow.description` | string? | What this flow does |
| `flow.keyboard_shortcut` | string? | Keyboard shortcut that triggers this flow (e.g., `"Cmd+K"`). Informational — auto-populated from `shortcut` trigger events by DDD Tool. Shown as badge at L2. |
| `flow.template` | boolean? | When `true`, this flow is a template (see Parameterized Flow pattern in Section 16) |
| `flow.parameters` | `Record<string, FlowParameter>`? | Template parameters (required when `template: true`) |
| `flow.contract` | object? | Sub-flow input/output contract (see below) |
| `flow.emits` | string[]? | Events this flow emits (informational — domain.yaml remains source of truth). DDD Tool auto-populates from event nodes. |
| `flow.listens_to` | string[]? | Events this flow consumes (informational — domain.yaml remains source of truth). DDD Tool auto-populates from trigger/event nodes. |
| `trigger` | DddFlowNode | The entry point node |
| `nodes` | DddFlowNode[] | All other nodes |
| `metadata` | object | `{ created, modified }` ISO timestamps |

### DddFlowNode (Common Fields)

Every node has:

| Field | Type | Description |
|-------|------|-------------|
| `id` | string | Unique node ID |
| `type` | DddNodeType | One of the 28 node types |
| `position` | `{ x, y }` | Canvas position |
| `connections` | Array | List of `{ targetNodeId, sourceHandle?, targetHandle?, data?, behavior?, label? }` |
| `spec` | object | Type-specific configuration (see below) |
| `label` | string | Display name on canvas |
| `observability` | object? | Logging/metrics/tracing config |
| `security` | object? | Auth/rate-limiting/encryption config |
| `parentId` | string? | ID of a container node (loop or parallel). When set, this node is rendered inside the container on the canvas. |

### Connection Data Annotations

Connections can optionally include a `data` field to document what data flows between nodes. This is purely descriptive — it doesn't affect behavior but enables data shape hover in the DDD Tool.

```yaml
connections:
  - targetNodeId: data_store-abc123
    sourceHandle: valid
    data:
      - { name: email, type: string }
      - { name: password, type: string }
      - { name: name, type: string }
```

### Connection Behavior

Connections can optionally include a `behavior` field to distinguish error handling strategies. This helps implementers understand how errors should propagate.

| Value | Description |
|-------|-------------|
| `continue` | Log and continue (soft fail) |
| `stop` | Stop the flow (hard fail) |
| `retry` | Retry before failing (retry count configured at node level) |
| `circuit_break` | Use circuit breaker pattern |

```yaml
connections:
  - targetNodeId: terminal-error-001
    sourceHandle: error
    behavior: continue
    label: "Log and skip"
```

### Connection Format Compatibility

The DDD Tool normalizes several alternative field names on import, so specs from different sources are accepted:

| You write | Normalized to |
|-----------|---------------|
| `target`, `targetId`, or `targetNodeId` | `targetNodeId` |
| `sourceHandle: "default"` | unnamed handle (omitted) |
| `spec`, `properties`, or `config` | `spec` |
| `label` or `name` | `label` |

You should prefer the canonical names (`targetNodeId`, `spec`, `label`) in new specs. The aliases exist for backward compatibility and interoperability with external generators.

### Variable Scope and Data Flow

DDD specs use `$.variable_name` syntax to reference data flowing through a flow. Here is how variable scope works:

**The `$` context object:**
- `$` represents the flow context — a mutable object created when the trigger fires and passed through every node in the flow.
- Each node reads inputs from `$` and writes its output back to `$` under a named key.
- Scope is **flow-scoped** and **accumulative**: later nodes can read anything written by earlier nodes.

**Lifecycle:**
1. **Trigger** initializes `$` from the incoming event payload (e.g., HTTP body, cron metadata, event data).
2. **Input** validates and may reshape fields on `$`.
3. **Process / decision / data_store / service_call** nodes read from `$` and write results back.
4. **Terminal** reads from `$` to build the response.

**Implementation mapping:**

| Spec Reference | Implementation (Express) | Implementation (Generic) |
|---------------|-------------------------|-------------------------|
| `$.field` in trigger (http) | `req.body.field` or `req.params.field` | Function argument field |
| `$.field` in trigger (event) | `event.payload.field` | Event data field |
| `$.model_name` after data_store read | `const model_name = await repo.find(...)` | Query result variable |
| `$.raw_*` (raw content) | Unparsed response body variable | Raw content variable |
| `$.result` in terminal | `res.json({ result })` | Return value |

**Naming conventions:**
- `$.raw_*` — unprocessed content (e.g., `$.raw_feed`, `$.raw_html`)
- `$.model_name` — data model instances (e.g., `$.order`, `$.user`)
- `$.model_name_list` — arrays of models (e.g., `$.order_list`)
- `$.is_*` or `$.has_*` — boolean flags for decision nodes (e.g., `$.is_valid`, `$.has_permission`)

**Code generation rule:** When generating code, replace each `$.X` reference with the corresponding local variable or parameter. The `$` object is a design-time abstraction — it does not need to exist as a literal runtime object.

### FlowParameter (for parameterized flows)

| Field | Type | Description |
|-------|------|-------------|
| `type` | string | Parameter type (`'string'`, `'integration_ref'`, `'number'`, `'boolean'`) |
| `values` | array? | Allowed values (for string type) |

---

## 6. All Node Types and Their Specs

### 6.1 Traditional Nodes

#### trigger
The entry point of every flow. Exactly one per flow.

| Spec Field | Type | Description |
|------------|------|-------------|
| `event` | string \| string[] | What triggers the flow. String or string[] for multi-event triggers (see conventions below) |
| `source` | string | Where the trigger comes from (e.g., "API Gateway") |
| `filter` | `Record<string, unknown>`? | Event payload filter — flow only triggers when filter matches (supports dot notation and operators) |
| `debounce_ms` | number? | Debounce delay in milliseconds. When set, `/ddd-implement` wraps the event handler in a debounce function. Only meaningful for `event:`, `ipc:`, `shortcut`, and `ui:` triggers. |
| `description` | string | Details |

**Trigger type conventions** — use these patterns in the `event` field to communicate trigger semantics:

```yaml
# HTTP trigger — "HTTP {METHOD} {path}"
spec:
  event: "HTTP POST /api/users/register"
  source: API Gateway
  description: User registration endpoint

# Scheduled trigger — "cron {expression}"
spec:
  event: "cron */30 * * * *"
  source: Scheduler
  description: Runs every 30 minutes

# Scheduled trigger with job configuration
spec:
  event: "cron */15 * * * *"
  source: Scheduler
  description: Check sources every 15 minutes
  job_config:
    queue: ingestion
    concurrency: 1
    timeout_ms: 300000
    retry:
      max_attempts: 3
      backoff_ms: 5000
    dead_letter: true
    lock_ttl_ms: 60000

# Event trigger — "event:{EventName}"
spec:
  event: "event:UserRegistered"
  source: Message Bus
  description: Triggered when a user registers

# Manual trigger
spec:
  event: "manual"
  source: Admin Dashboard
  description: Triggered manually by admin

# Webhook trigger — "webhook {path}"
spec:
  event: "webhook /stripe/events"
  source: Stripe
  description: Stripe webhook callback

# Multi-event trigger — string array
spec:
  event:
    - "event:ContentDiscovered"
    - "event:SourceContentFound"
  source: Message Bus
  description: Triggered by either content discovery or monitoring

# Event group trigger — "event_group:{name}" (references event_groups in domain.yaml)
spec:
  event: "event_group:audit_events"
  source: Message Bus
  description: Triggered by any event in the audit_events group

# SSE trigger — Server-Sent Events endpoint
spec:
  event: "sse /api/updates"
  source: API Gateway
  description: Real-time update stream for dashboard

# WebSocket trigger
spec:
  event: "ws /api/live"
  source: API Gateway
  description: Bidirectional real-time connection

# Event pattern trigger — fires when a pattern of events occurs
spec:
  event: "pattern:ContentAnalyzed"
  source: Event Aggregator
  description: Trigger when 3+ relevant items from same author within 7 days
  pattern:
    event: ContentAnalyzed
    group_by: author_name
    threshold: 3
    window: "7d"

# Event trigger with payload filter — only trigger when filter matches
spec:
  event: "event:DraftApproved"
  source: approval-domain
  filter:
    platform: twitter
  description: Only triggers when payload.platform is "twitter"

# Filter with operators
spec:
  event: "event:OrderCreated"
  source: orders-domain
  filter:
    "payload.amount": { gte: 100 }
    "payload.status": [pending, confirmed]
  description: Only high-value pending/confirmed orders

# Desktop: keyboard shortcut trigger — "shortcut {keys}"
spec:
  event: "shortcut Cmd+K"
  source: Keyboard
  description: Open search palette

# Desktop: timer/interval trigger — "timer {interval_ms}"
spec:
  event: "timer 10000"
  source: Timer
  description: Poll for changes every 10 seconds

# Desktop: UI action trigger — "ui:{action}"
spec:
  event: "ui:DragDrop"
  source: Canvas
  description: Triggered when user drags and drops on canvas

# Desktop: IPC event trigger — "ipc:{event}"
spec:
  event: "ipc:spec-files-changed"
  source: File Watcher
  description: Triggered when native file watcher detects spec changes
```

**job_config** — optional fields for cron triggers to configure the job queue:

| Field | Type | Description |
|-------|------|-------------|
| `queue` | string? | Named queue for the job (e.g., "ingestion", "analytics") |
| `concurrency` | number? | Max concurrent executions (default: 1) |
| `timeout_ms` | number? | Job timeout in milliseconds |
| `retry` | object? | `{ max_attempts?, backoff_ms?, strategy?: 'fixed' \| 'linear' \| 'exponential', jitter?: boolean }` |
| `dead_letter` | boolean? | Send failed jobs to dead letter queue |
| `lock_ttl_ms` | number? | Distributed lock TTL to prevent overlapping runs |
| `jitter_ms` | number? | Random delay (0 to jitter_ms) before job starts, for staggered execution |
| `priority` | number? | Job priority (higher = processed first) |
| `dedup_key` | string? | Deduplication key template (e.g., "source:{sourceId}") |

#### input
Validates incoming data. Has two output handles: `valid` and `invalid`. Always specify `sourceHandle` on connections.

| Spec Field | Type | Description |
|------------|------|-------------|
| `fields` | `Array<{ name, type, required? }>` | Field definitions |
| `validation` | string | Validation rules/regex |
| `description` | string | What this input represents |

**Conditional required fields** — use the `validation` string to express conditional rules:

```yaml
spec:
  fields:
    - name: action
      type: string
      required: true
    - name: reason
      type: string
      required: false
  validation: "required_if: action == 'reject' then reason is required"
  description: Review decision
```

#### process
Business logic step. Use specialized node types (collection, parse, crypto, batch, transaction) when they fit — reserve process for genuine business logic.

| Spec Field | Type | Description |
|------------|------|-------------|
| `action` | string | What this step does |
| `service` | string | Service/function to call |
| `category` | string? | `'security' \| 'transform' \| 'integration' \| 'business_logic' \| 'infrastructure'` — helps implementers understand what kind of code to write |
| `inputs` | string[]? | Explicit input fields (e.g., `["$.password"]`) |
| `outputs` | string[]? | Explicit output fields (e.g., `["$.hashed_password"]`) |
| `description` | string | Details |

#### decision
Branching point. Has two output handles: `true` and `false`. Connections must use `sourceHandle: "true"` or `sourceHandle: "false"`.

| Spec Field | Type | Description |
|------------|------|-------------|
| `condition` | string | The condition to evaluate |
| `trueLabel` | string | Label for the true branch |
| `falseLabel` | string | Label for the false branch |
| `description` | string | Details |

#### terminal
End state. Must have zero outgoing connections. Use custom fields `status` and `body` to specify the HTTP response for implementation.

| Spec Field | Type | Description |
|------------|------|-------------|
| `outcome` | string | Result (e.g., "success", "error", "timeout") |
| `description` | string | What happens at this endpoint |
| `status` | number? | HTTP status code (custom field for implementation) |
| `body` | object? | Response body shape (custom field for implementation) |
| `response_type` | string? | `'json' \| 'stream' \| 'sse' \| 'empty'` (default: 'json') |
| `headers` | `Record<string, string>?` | Custom response headers |

**Terminal response examples:**

```yaml
# Success response with data
spec:
  outcome: success
  description: Return user object with JWT token
  status: 201
  body:
    message: "User registered successfully"
    user: "$.user"
    token: "$.jwt_token"

# Error response with error code
spec:
  outcome: error
  description: Return 409 Conflict
  status: 409
  body:
    error: "DUPLICATE_ENTRY"
    message: "Email already registered"

# Empty success (no body)
spec:
  outcome: success
  description: Delete completed
  status: 204

# Streaming response
spec:
  outcome: success
  description: Stream audio from ElevenLabs TTS
  response_type: stream
  status: 200
  headers:
    Content-Type: audio/mpeg
    Transfer-Encoding: chunked
```

### 6.2 Extended Traditional Nodes

#### data_store
Data storage operation. Supports databases, filesystem, and in-memory stores. Use `sourceHandle` values `"success"` and `"error"` on connections for success/error routing.

| Spec Field | Type | Values |
|------------|------|--------|
| `store_type` | string? | `'database' \| 'filesystem' \| 'memory'` — defaults to `'database'` if omitted (backwards compatible) |
| `operation` | string | Database: `'create' \| 'read' \| 'update' \| 'delete' \| 'upsert' \| 'create_many' \| 'update_many' \| 'delete_many'`. Memory: `'get' \| 'set' \| 'merge' \| 'reset' \| 'subscribe' \| 'update_where'` (CRUD aliases also accepted) |
| `model` | string | Entity/table name (database) |
| `data` | `Record<string, string>` | Fields to write (for create/update) |
| `query` | `Record<string, string>` | Query conditions (for read/update/delete) |
| `description` | string | Details |
| `pagination` | object? | Pagination config (for list operations) |
| `sort` | object? | Sort config (for list operations) |
| `batch` | boolean? | Batch operation (process entire collection in one DB call) |
| `upsert_key` | string[]? | Fields for upsert conflict resolution (required when operation is 'upsert') |
| `include` | IncludeRelation[]? | Eager-load related records (joins) |
| `returning` | boolean? | Return affected records (for create_many/update_many/delete_many) |
| `safety` | string? | `'strict' \| 'lenient'` — when `'strict'`, `/ddd-implement` wraps all property accesses from this read in null-safe patterns (optional chaining, default values). Default: `'strict'`. |

**Filesystem fields** (when `store_type: 'filesystem'`):

| Field | Type | Description |
|-------|------|-------------|
| `path` | string | File path (supports `$.` variable interpolation) |
| `content` | string | Content to write (for create/update operations) |
| `create_parents` | boolean? | Create parent directories if they don't exist |

```yaml
# Filesystem: read a YAML config file
spec:
  store_type: filesystem
  operation: read
  path: "$.project_path/specs/system.yaml"
  description: Load system spec from disk

# Filesystem: write a file with parent directory creation
spec:
  store_type: filesystem
  operation: create
  path: "$.output_dir/reports/summary.json"
  content: "$.report_json"
  create_parents: true
  description: Write generated report to disk
```

**Memory fields** (when `store_type: 'memory'`):

| Field | Type | Description |
|-------|------|-------------|
| `store` | string | Store name (e.g., "project-store", "flow-store") |
| `selector` | string | State path to read/write (e.g., "domains", "currentFlow.nodes") |

**Memory operations** (preferred when `store_type: 'memory'`):

| Operation | Equivalent CRUD | Description |
|-----------|----------------|-------------|
| `get` | `read` | Read state at selector path |
| `set` | `create`/`update` | Replace state at selector path |
| `merge` | `update` | Shallow-merge into state at selector path |
| `reset` | `delete` | Reset store to initial state (selector optional) |
| `subscribe` | — | Reactive subscription to state changes at selector path |
| `update_where` | — | Find items in array by predicate and apply patch |

**Additional `update_where` fields:**

| Field | Type | Description |
|-------|------|-------------|
| `predicate` | string | Expression to match items (e.g., `"$.id === targetId"`) |
| `patch` | `Record<string, unknown>` | Fields to update on matched items |

```yaml
# Memory: read from an in-memory store
spec:
  store_type: memory
  operation: get
  store: project-store
  selector: domains
  description: Read domains from project store

# Memory: update specific item in array by predicate
spec:
  store_type: memory
  operation: update_where
  store: app-store
  selector: errors
  predicate: "$.id === targetErrorId"
  patch:
    dismissed: true
  description: Dismiss a specific error by ID
```

**Pagination and sorting** — for read operations that return lists, add pagination custom fields:

```yaml
spec:
  operation: read
  model: Product
  query:
    status: active
  pagination:
    style: cursor
    default_limit: 20
    max_limit: 100
  sort:
    default: "created_at:desc"
    allowed: ["created_at", "name", "price"]
  description: List active products with pagination
```

**Include (joins)** — eager-load related records instead of multiple sequential reads:

| Field | Type | Description |
|-------|------|-------------|
| `model` | string | Related entity name |
| `via` | string | Foreign key or relation path (supports dot notation for nested) |
| `as` | string | Output field name |

```yaml
# Read with joined relations
spec:
  operation: read
  model: Draft
  query: { id: "$.draft_id" }
  include:
    - model: ContentItem
      via: content_item_id
      as: content_item
    - model: SocialAccount
      via: content_item.subject_id
      as: social_account
  description: Load draft with content item and social account
```

**Bulk operations:**

```yaml
# Update many records
spec:
  operation: update_many
  model: ApiKey
  query: { user_id: "$.user_id" }
  data: "$.validation_results"
  returning: true
  description: Update validation status for all user API keys
```

#### service_call
External API call. Use `sourceHandle` values `"success"` and `"error"` on connections for success/error routing. The `error_mapping` field maps HTTP status codes to error code strings that `/ddd-implement` will use in error handlers.

| Spec Field | Type | Values |
|------------|------|--------|
| `method` | string | `'GET' \| 'POST' \| 'PUT' \| 'PATCH' \| 'DELETE'` |
| `url` | string | API endpoint URL |
| `headers` | `Record<string, string>` | HTTP headers |
| `body` | `Record<string, unknown>` | Request body |
| `timeout_ms` | number | Request timeout |
| `retry` | object | `{ max_attempts?, backoff_ms?, strategy?: 'fixed' \| 'linear' \| 'exponential', jitter?: boolean }` |
| `error_mapping` | `Record<string, string>` | Status code to error code mapping |
| `request_config` | RequestConfig? | Outbound request behavior configuration |
| `integration` | string? | Reference to integration defined in system.yaml |
| `description` | string | Details |

**Error mapping example:**

```yaml
spec:
  method: POST
  url: "https://payment.internal/api/charge"
  error_mapping:
    "400": "INVALID_PAYMENT_DATA"
    "402": "PAYMENT_DECLINED"
    "409": "DUPLICATE_CHARGE"
    "503": "PAYMENT_SERVICE_UNAVAILABLE"
  description: Charge customer payment method
```

**RequestConfig** — configures outbound HTTP client behavior:

| Field | Type | Description |
|-------|------|-------------|
| `user_agent` | string? | `'rotate' \| 'browser' \| 'custom'` — User-Agent strategy |
| `delay` | object? | `{ min_ms, max_ms, strategy: 'random' \| 'fixed' }` — inter-request delay |
| `cookie_jar` | string? | `'per_domain' \| 'shared' \| 'none'` |
| `proxy` | string? | `'pool' \| 'direct' \| 'tor'` |
| `tls_fingerprint` | string? | `'randomize' \| 'chrome' \| 'firefox' \| 'default'` |
| `fallback` | string? | `'headless_browser' \| 'none'` — fallback for bot-resistant sites |

#### ipc_call
Local IPC or native function call (Tauri commands, Electron IPC, React Native bridge, etc.). Use `sourceHandle` values `"success"` and `"error"` on connections for success/error routing.

| Spec Field | Type | Description |
|------------|------|-------------|
| `command` | string | Command/function name (e.g., "git_status", "compute_file_hash") |
| `args` | `Record<string, unknown>`? | Named arguments to pass to the command |
| `return_type` | string? | Expected return type (e.g., "string", "GitStatus", "boolean") |
| `timeout_ms` | number? | Call timeout in milliseconds |
| `bridge` | string? | Target bridge/runtime (e.g., "tauri", "electron", "react-native") — for implementation targeting |
| `result_condition` | string? | Expression that maps return values to success/error handles (e.g., `"$.result === true"` → success, else → error). Eliminates need for a separate decision node after boolean-returning calls. |
| `description` | string | Details |

```yaml
# Call a Tauri command
spec:
  command: git_status
  args:
    repo_path: "$.project_path"
  return_type: GitStatus
  timeout_ms: 5000
  bridge: tauri
  description: Get git status for the project repository

# Compute file hash
spec:
  command: compute_file_hash
  args:
    path: "$.file_path"
    algorithm: sha256
  return_type: string
  bridge: tauri
  description: Compute SHA-256 hash of the spec file

# Boolean IPC with result_condition — routes to success/error without a decision node
spec:
  command: path_exists
  args:
    path: "$.target_path"
  return_type: boolean
  result_condition: "$.result === true"
  bridge: tauri
  description: Check if the target path exists
```

#### event
Publish or subscribe to an event.

| Spec Field | Type | Values |
|------------|------|--------|
| `direction` | string | `'emit' \| 'consume'` |
| `event_name` | string | Event identifier |
| `payload` | `Record<string, unknown>` | Event data shape (static template, also serves as schema documentation) |
| `payload_source` | string? | Dynamic payload variable reference (overrides static `payload` when present) |
| `async` | boolean | Fire-and-forget? |
| `target_queue` | string? | Specific queue name (when using as job enqueue vs domain event) |
| `priority` | number? | Job priority when enqueueing |
| `delay_ms` | number? | Delay before processing |
| `dedup_key` | string? | Deduplication key |
| `description` | string | Details |

#### loop
Iterate over a collection. Has two output paths: `"body"` for the loop body and `"done"` for after the loop completes. Use `sourceHandle` values on connections.

| Spec Field | Type | Description |
|------------|------|-------------|
| `collection` | string | What to iterate (e.g., "users", "$.items") |
| `iterator` | string | Iterator variable name |
| `break_condition` | string | When to stop early |
| `on_error` | string? | `'continue' \| 'break' \| 'fail'` — behavior when iteration fails (default: 'fail') |
| `accumulate` | AccumulateConfig? | Collect results across iterations |
| `body_start` | string? | Node ID of the first node in the loop body. Used for nested layout — nodes with `parentId` matching this loop's ID are rendered inside the loop container. |
| `description` | string | Details |

**AccumulateConfig fields:**

| Field | Type | Description |
|-------|------|-------------|
| `field` | string | What to collect from each iteration's output |
| `strategy` | string | `'append' \| 'merge' \| 'sum' \| 'last'` |
| `output` | string | Variable name available at `"done"` handle |

```yaml
# Loop with result accumulation
spec:
  collection: "$.sources"
  iterator: source
  on_error: continue
  accumulate:
    field: new_posts
    strategy: append
    output: "all_new_posts"
  description: Check each source and collect new posts
```

**Nested loop layout** — nodes inside a loop body can declare `parentId` to be visually nested inside the loop container on the canvas:

```yaml
# Loop node
- id: loop-abc123
  type: loop
  spec:
    collection: "$.items"
    iterator: item
    body_start: process-def456
    description: Process each item

# Node inside the loop body
- id: process-def456
  type: process
  parentId: loop-abc123    # ← renders inside the loop container
  spec:
    action: validate_item
    description: Validate the current item
```

The `parentId` field is a common field available on any `DddFlowNode`. When set, the node is rendered as a child of the container node (loop or parallel) on the canvas. The `body_start` field on the loop spec tells the DDD Tool which node begins the loop body for layout purposes.

#### parallel
Run branches concurrently. The `branches` field documents what each branch does and can optionally include conditions to skip branches. Actual parallel paths are defined by connections using `sourceHandle: "branch-0"`, `"branch-1"`, etc. Use `sourceHandle: "done"` for the join/continuation node.

| Spec Field | Type | Values |
|------------|------|--------|
| `branches` | `ParallelBranch[]` | Branch definitions (see below). Also accepts `string[]` for simple cases, or a `number` (e.g., `branches: 3`) — the DDD Tool coerces numbers into generated labels (`"Branch 1"`, `"Branch 2"`, etc.) |
| `join` | string | `'all' \| 'any' \| 'n_of'` |
| `join_count` | number | Required if join is `n_of` |
| `timeout_ms` | number | Max wait time |
| `description` | string | Details |

**ParallelBranch fields:**

| Field | Type | Description |
|-------|------|-------------|
| `id` | string? | Branch identifier |
| `label` | string | Branch description |
| `condition` | string? | Skip branch if condition evaluates to false |

```yaml
# Conditional branches — only run branches where condition is true
spec:
  branches:
    - id: twitter-draft
      label: "Generate Twitter draft"
      condition: "$.social_accounts.has_twitter"
    - id: linkedin-draft
      label: "Generate LinkedIn draft"
      condition: "$.social_accounts.has_linkedin"
  join: all
  description: Generate drafts for available platforms
```

#### sub_flow
Reference another flow.

| Spec Field | Type | Description |
|------------|------|-------------|
| `flow_ref` | string | Target flow in `domain/flow-id` format |
| `input_mapping` | `Record<string, string>` | Input parameter mapping |
| `output_mapping` | `Record<string, string>` | Output parameter mapping |
| `description` | string | Details |

**Sub-flow contracts** — flows that are called as sub-flows can define a `contract` in their flow metadata:

```yaml
# In the flow YAML that acts as a sub-flow:
flow:
  id: normalize-content
  name: Normalize Content
  type: traditional
  domain: ingestion
  description: Normalize raw content into standard format
  contract:
    inputs:
      - name: raw_content
        type: string
        required: true
      - name: platform
        type: enum
        ref: platform
        required: true
    outputs:
      - name: content_item_id
        type: uuid
      - name: is_duplicate
        type: boolean
```

| Contract Field | Type | Description |
|----------------|------|-------------|
| `inputs` | `Array<{ name, type, required?, ref? }>` | Expected input parameters |
| `outputs` | `Array<{ name, type }>` | Returned output values |

> **Note:** The `contract` section is optional. When present, `/ddd-implement` validates that `sub_flow` nodes' `input_mapping`/`output_mapping` match the target flow's contract.

#### delay
Introduce a deliberate wait before continuing. Useful for rate limiting, anti-bot patterns, or scheduled pauses.

| Spec Field | Type | Description |
|------------|------|-------------|
| `min_ms` | number | Minimum delay in milliseconds |
| `max_ms` | number | Maximum delay (if strategy is random) |
| `strategy` | string | `'fixed' \| 'random'` |
| `description` | string | Why this delay exists |

#### cache
Check cache before expensive operations. Has two output handles: `"hit"` and `"miss"`. Use `sourceHandle` on connections.

| Spec Field | Type | Description |
|------------|------|-------------|
| `key` | string | Cache key template (e.g., "search:{query}") |
| `ttl_ms` | number | Time-to-live in milliseconds |
| `store` | string | `'redis' \| 'memory'` |
| `description` | string | What is being cached |

#### transform
Structured data mapping between formats. Prefer over process nodes when the operation is pure data transformation.

| Spec Field | Type | Description |
|------------|------|-------------|
| `input_schema` | string | Source data format reference |
| `output_schema` | string | Target data format reference |
| `field_mappings` | `Record<string, string>` | Output field to input field/expression mapping |
| `description` | string | What transformation is performed |

#### llm_call
Single LLM invocation (not an agent loop).

| Spec Field | Type | Description |
|------------|------|-------------|
| `model` | string | Model ID (e.g., "claude-sonnet") |
| `system_prompt` | string | System instructions |
| `prompt_template` | string | Prompt with `{variable}` placeholders |
| `temperature` | number | 0.0 - 1.0 |
| `max_tokens` | number | Max output tokens |
| `structured_output` | `Record<string, unknown>` | Expected output schema |
| `retry` | object | `{ max_attempts?, backoff_ms?, strategy?: 'fixed' \| 'linear' \| 'exponential', jitter?: boolean }` |
| `context_sources` | `Record<string, ContextSource>`? | Formal variable bindings with optional transforms |
| `description` | string | Details |

**ContextSource fields:**

| Field | Type | Description |
|-------|------|-------------|
| `from` | string | Variable path (e.g., `"$.content_item.raw_content"`) |
| `transform` | string? | Transform function to apply before injection |

**Supported transforms:** `truncate(n)`, `join(sep)`, `lowercase`, `uppercase`, `first(n)`, `last(n)`, `json_stringify`, `strip_html`, `summarize(n)`

```yaml
# LLM call with explicit context bindings
spec:
  model: claude-sonnet
  system_prompt: "You are a social media expert."
  prompt_template: "Write a {platform} post about: {content}"
  context_sources:
    platform:
      from: "$.social_account.platform"
    content:
      from: "$.content_item.raw_content"
      transform: truncate(4000)
    keywords:
      from: "$.subject.keywords"
      transform: join(", ")
```

#### collection
Collection operations: filter, sort, deduplicate, merge, group_by, aggregate, reduce, flatten. Use instead of process nodes for any collection transformation. Has two output handles: `"result"` (the transformed collection) and `"empty"` (collection is empty after operation).

| Spec Field | Type | Description |
|------------|------|-------------|
| `operation` | string | `'filter' \| 'sort' \| 'deduplicate' \| 'merge' \| 'group_by' \| 'aggregate' \| 'reduce' \| 'flatten'` |
| `input` | string | Input collection reference (e.g., `"$.sources"`) |
| `predicate` | string? | Filter expression (for `filter`) |
| `key` | string? | Field key (for `deduplicate`, `sort`, `group_by`) |
| `direction` | string? | `'asc' \| 'desc'` (for `sort`) |
| `accumulator` | object? | `{ init: any, expression: string }` (for `reduce`/`aggregate`) |
| `output` | string | Output variable name |
| `description` | string | Details |

> **Iterator scoping:** Inside collection operations, `item.` refers to the current element being iterated. This is distinct from `$.` which references the flow-scoped context. For example, `item.url` accesses a field on the current collection element, while `$.last_checked_at` reads from the flow context. In `reduce` operations, `acc` refers to the accumulator value.

```yaml
# Filter example
spec:
  operation: filter
  input: "$.entries"
  predicate: "item.published_at > $.last_checked_at"
  output: "new_entries"

# Deduplicate example
spec:
  operation: deduplicate
  input: "$.merged_results"
  key: "item.url"
  output: "unique_results"

# Reduce example
spec:
  operation: reduce
  input: "$.batched_items"
  accumulator:
    init: []
    expression: "acc.concat(item.new_posts)"
  output: "all_posts"
```

#### parse
Structured extraction from raw content formats (RSS, HTML, XML, JSON, CSV). Use instead of process nodes for parsing. Has two output handles: `"success"` and `"error"` (malformed content).

| Spec Field | Type | Description |
|------------|------|-------------|
| `format` | string | `'rss' \| 'atom' \| 'html' \| 'xml' \| 'json' \| 'csv' \| 'markdown'` |
| `input` | string | Raw content variable (e.g., `"$.raw_response"`) |
| `strategy` | string \| object | String: `'strict' \| 'lenient' \| 'streaming'` for parsing strictness (default: `'strict'`). Object: `{ selectors: [...] }` for HTML scraping with CSS selectors. |
| `library` | string? | Implementation hint (e.g., `'cheerio'`, `'rss-parser'`, `'fast-xml-parser'`) |
| `output` | string | Output variable name |
| `description` | string | Details |

```yaml
# RSS parsing
spec:
  format: rss
  input: "$.raw_feed"
  strategy: lenient
  output: "feed_entries"

# HTML scraping
spec:
  format: html
  input: "$.page_html"
  strategy:
    selectors:
      - { name: title, css: "h1.article-title" }
      - { name: body, css: "article.content", extract: text }
      - { name: links, css: "a[href]", extract: href, multiple: true }
  library: cheerio
  output: "extracted_data"
```

#### crypto
Cryptographic operations: encrypt, decrypt, hash, sign, verify, generate_key. Use instead of process nodes for any security operation. Has two output handles: `"success"` and `"error"` (crypto failure).

| Spec Field | Type | Description |
|------------|------|-------------|
| `operation` | string | `'encrypt' \| 'decrypt' \| 'hash' \| 'sign' \| 'verify' \| 'generate_key'` |
| `algorithm` | string | `'aes-256-gcm' \| 'aes-256-cbc' \| 'sha256' \| 'sha512' \| 'hmac-sha256' \| 'rsa-oaep' \| 'ed25519'` |
| `key_source` | object | Where the key comes from: `{ env: string }` |
| `input_fields` | string[] | Field(s) to process |
| `output_field` | string | Result field name |
| `encoding` | string? | `'base64' \| 'hex'` — output encoding |
| `description` | string | Details |

```yaml
spec:
  operation: encrypt
  algorithm: aes-256-gcm
  key_source: { env: "ENCRYPTION_KEY" }
  input_fields: ["api_key"]
  output_field: "encrypted_key"
  encoding: base64
  description: Encrypt API key for storage
```

#### batch
Execute a heterogeneous list of operations against a collection where each item may target a different endpoint, auth method, or response format. Has two output handles: `"done"` (all complete) and `"error"` (if `on_error: stop` and one fails).

| Spec Field | Type | Description |
|------------|------|-------------|
| `input` | string | Collection to iterate over (e.g., `"$.api_keys"`) |
| `operation_template` | object | Per-item operation config (see below) |
| `concurrency` | number? | Max parallel operations |
| `on_error` | string? | `'continue' \| 'stop'` — per-item error handling |
| `output` | string | Output variable (array of `{ item, result, success }`) |
| `description` | string | Details |

**OperationTemplate fields:**

| Field | Type | Description |
|-------|------|-------------|
| `type` | string | Operation type (`'service_call'`, `'data_store'`, etc.) |
| `dispatch_field` | string | Field that determines which config to use |
| `configs` | `Record<string, object>` | Named configs keyed by dispatch value |

```yaml
spec:
  input: "$.api_keys"
  operation_template:
    type: service_call
    dispatch_field: "item.provider"
    configs:
      openai:
        method: GET
        url: "https://api.openai.com/v1/models"
        headers: { Authorization: "Bearer {item.key}" }
        success_check: "response.status == 200"
      anthropic:
        method: GET
        url: "https://api.anthropic.com/v1/models"
        headers: { x-api-key: "{item.key}" }
        success_check: "response.status == 200"
  concurrency: 3
  on_error: continue
  output: "validation_results"
  description: Validate API keys via lightweight test calls
```

#### transaction
Atomic multi-step database operations. Groups multiple data operations into a unit — if any step fails, all are rolled back. Has two output handles: `"committed"` and `"rolled_back"`.

| Spec Field | Type | Description |
|------------|------|-------------|
| `isolation` | string? | `'read_committed' \| 'serializable' \| 'repeatable_read'` |
| `steps` | array | Ordered operations: `[{ action, rollback? }]` |
| `rollback_on_error` | boolean? | Auto-rollback on any step failure (default: true) |
| `description` | string | Details |

**Step fields:**

| Field | Type | Description |
|-------|------|-------------|
| `action` | string | What this step does (e.g., "create source record", "update creator status") |
| `rollback` | string? | How to undo this step on failure (e.g., "delete created source") |

```yaml
spec:
  isolation: read_committed
  steps:
    - { action: "Create source record", rollback: "Delete created source" }
    - { action: "Update creator promoted status", rollback: "Revert creator promoted to false" }
  rollback_on_error: true
  description: Create source and update creator atomically
```

### 6.3 Agent Nodes (for `type: agent` flows)

#### agent_loop
LLM agent with tools in a loop.

| Spec Field | Type | Values |
|------------|------|--------|
| `model` | string | Model ID |
| `system_prompt` | string | Agent instructions |
| `max_iterations` | number | Max tool-use loops |
| `temperature` | number | 0.0 - 1.0 |
| `stop_conditions` | string[] | When to stop (e.g., "answer_provided") |
| `tools` | ToolDefinition[] | Available tools (see below) |
| `memory` | MemoryStoreDefinition[] | Memory stores (see below) |
| `on_max_iterations` | string | `'escalate' \| 'respond' \| 'error'` |

**ToolDefinition:**

| Field | Type | Description |
|-------|------|-------------|
| `id` | string | Unique tool ID |
| `name` | string | Function name |
| `description` | string | What the tool does |
| `parameters` | string | JSON schema of parameters |
| `implementation` | string? | Implementation reference |
| `is_terminal` | boolean? | Ends the loop when used |
| `requires_confirmation` | boolean? | Needs human approval |

**MemoryStoreDefinition:**

| Field | Type | Values |
|-------|------|--------|
| `name` | string | Store name |
| `type` | string | `'conversation_history' \| 'vector_store' \| 'key_value'` |
| `max_tokens` | number? | Token limit |
| `strategy` | string? | Eviction strategy (e.g., "sliding_window") |

#### guardrail
Input/output validation for agent flows. Guardrails are **inline and sequential** — data flows through them in order. They are NOT sidecars or parallel watchers. Place them before or after an agent_loop in the connection chain.

| Spec Field | Type | Values |
|------------|------|--------|
| `position` | string | `'input' \| 'output'` |
| `checks` | GuardrailCheck[] | `{ type: string, action: 'block' \| 'warn' \| 'log' }` |
| `on_block` | string | What to do when blocked |

#### human_gate
Asynchronous human approval step for agent workflows. The agent pauses and waits for human review before continuing.

**Important:** `human_gate` is for async agent approval workflows (e.g., "agent proposes resolution, human reviews"). For synchronous HTTP request/response approval patterns (e.g., "admin approves a user request"), use an HTTP trigger + decision node instead. See Section 16 Design Patterns.

| Spec Field | Type | Description |
|------------|------|-------------|
| `notification_channels` | string[] | e.g., ["slack", "email"] |
| `approval_options` | ApprovalOption[] | `{ id, label, description?, requires_input? }` |
| `timeout` | object | `{ duration?: number, action?: 'escalate' \| 'auto_approve' \| 'auto_reject' }` |
| `context_for_human` | string[] | Data to show the human reviewer |

### 6.4 Orchestration Nodes

#### orchestrator
Manages multiple agents.

| Spec Field | Type | Values |
|------------|------|--------|
| `strategy` | string | `'supervisor' \| 'round_robin' \| 'broadcast' \| 'consensus'` |
| `model` | string | Supervisor model |
| `supervisor_prompt` | string | Instructions for the supervisor |
| `agents` | OrchestratorAgent[] | `{ id, flow, specialization?, priority? }` |
| `fallback_chain` | string[] | Fallback agent order |
| `shared_memory` | SharedMemoryEntry[] | `{ name, type, access: 'read_write' \| 'read_only' }` |
| `supervision` | object | `{ monitor_iterations?, intervene_on?: SupervisionRule[] }` |
| `result_merge_strategy` | string | `'last_wins' \| 'best_of' \| 'combine' \| 'supervisor_picks'` |

#### smart_router
Routes to different agents based on rules.

| Spec Field | Type | Description |
|------------|------|-------------|
| `rules` | SmartRouterRule[] | `{ id, condition, route, priority? }` |
| `llm_routing` | object | `{ enabled?, model?, routing_prompt?, confidence_threshold?, routes? }` |
| `fallback_chain` | string[] | Fallback route order |
| `policies` | object | retry, timeout, circuit_breaker configs |

#### handoff
Transfer control between agents.

| Spec Field | Type | Values |
|------------|------|--------|
| `mode` | string | `'transfer' \| 'consult' \| 'collaborate'` |
| `target` | object | `{ flow?, domain? }` |
| `context_transfer` | object | `{ include_types?, max_context_tokens? }` |
| `on_complete` | object | `{ return_to?, merge_strategy? }` |
| `on_failure` | object | `{ action?, timeout? }` |
| `notify_customer` | boolean | Notify the end user? |

#### agent_group
Group of agents working together.

| Spec Field | Type | Description |
|------------|------|-------------|
| `name` | string | Group name |
| `description` | string | What this group does |
| `members` | AgentGroupMember[] | `{ flow, domain? }` |
| `shared_memory` | SharedMemoryEntry[] | Shared state |
| `coordination` | object | `{ communication?, max_active_agents?, selection_strategy?, sticky_session? }` |

---

## 7. Cross-Cutting Concerns (Per-Node)

Every node can optionally have `observability` and `security` configs:

### Observability

```yaml
observability:
  logging:
    level: info          # debug | info | warn | error
    include_input: true
    include_output: false
  metrics:
    enabled: true
    custom_counters:
      - requests
      - errors
  tracing:
    enabled: true
    span_name: node.validate_input
```

### Security

```yaml
security:
  authentication:
    required: true
    methods: [jwt, api_key]
    roles: [admin, user]
  rate_limiting:
    enabled: true
    requests_per_minute: 60
    key: ip
  encryption:
    at_rest: true
    in_transit: true
    pii_fields: [email, ssn]
  audit:
    enabled: true
```

**`rate_limiting.key`** — specifies what to rate-limit by: `'ip'` (default) | `'api_key'` | `'user'` | `'custom'`. When omitted, implementations should default to IP-based rate limiting.

---

## 8. Connection Patterns

Connections define the flow graph. Each connection is `{ targetNodeId, sourceHandle?, targetHandle?, data?, behavior?, label? }` (see Section 5 for `data`, `behavior`, and `label` details).

**Convention:** For nodes with multiple output paths, always use `sourceHandle` to label each path. This makes the flow graph unambiguous for both the DDD Tool and `/ddd-implement`.

### sourceHandle Reference

| Node Type | Handle A | Handle B | Notes |
|-----------|----------|----------|-------|
| `input` | `"valid"` | `"invalid"` | |
| `decision` | `"true"` | `"false"` | |
| `data_store` | `"success"` | `"error"` | |
| `service_call` | `"success"` | `"error"` | |
| `ipc_call` | `"success"` | `"error"` | |
| `llm_call` | `"success"` | `"error"` | Unnamed handle = `"success"` |
| `loop` | `"body"` | `"done"` | |
| `parallel` | `"branch-0"`, `"branch-1"`, ... | `"done"` | |
| `cache` | `"hit"` | `"miss"` | |
| `collection` | `"result"` | `"empty"` | |
| `parse` | `"success"` | `"error"` | |
| `crypto` | `"success"` | `"error"` | |
| `batch` | `"done"` | `"error"` | |
| `transaction` | `"committed"` | `"rolled_back"` | |
| `guardrail` | `"pass"` | `"block"` | Also accepts `"valid"`/`"invalid"` as aliases |
| `agent_loop` | `"done"` | `"error"` | |
| `smart_router` | dynamic route IDs | | From `rules[].id` |
| `human_gate` | dynamic option IDs | | From `approval_options[].id` |
| All others | *(single unnamed output)* | | process, delay, transform, sub_flow, orchestrator, handoff, agent_group, event, terminal |

### Examples

**Binary branching** (decision, input, data_store, service_call, etc.):
```yaml
connections:
  - targetNodeId: success-node-id
    sourceHandle: "true"
  - targetNodeId: failure-node-id
    sourceHandle: "false"
```

**Multi-branch** (parallel):
```yaml
connections:
  - targetNodeId: branch-a-first-node
    sourceHandle: "branch-0"
  - targetNodeId: branch-b-first-node
    sourceHandle: "branch-1"
  - targetNodeId: join-node-id
    sourceHandle: "done"
```

**Dynamic handles** (human_gate — handles from `approval_options[].id`):
```yaml
connections:
  - targetNodeId: approved-path-id
    sourceHandle: "approve"
  - targetNodeId: rejected-path-id
    sourceHandle: "reject"
```

**Single output** (process, delay, transform, sub_flow, orchestrator, handoff, agent_group):
```yaml
connections:
  - targetNodeId: next-node-id
```

---

## 9. Validation Rules

The DDD Tool enforces these validation rules. Your specs should pass all of them.

### Flow-Level (Error)
- Every flow must have exactly one trigger node
- All paths from trigger must reach a terminal node
- No orphaned (unreachable) nodes
- No circular paths in traditional flows (agents may have cycles)
- Trigger must have an `event` defined
- Input fields must have `type` defined
- Decision must have a `condition` defined
- Branching nodes must have all output paths wired (see Section 8 for handles per node type):
  - Decision: both `true` and `false` branches
  - Input: both `valid` and `invalid` paths
  - Data Store, Service Call, IPC Call, LLM Call, Parse, Crypto: both `success` and `error` paths
  - Loop: both `body` and `done` paths
  - Parallel: all `branch-N` paths plus `done`
  - Cache: both `hit` and `miss` paths
  - Collection: both `result` and `empty` paths
  - Guardrail: both `pass` and `block` paths
  - Agent Loop, Batch: both `done` and `error` paths
  - Transaction: both `committed` and `rolled_back` paths
  - Smart Router: connections for each `rules[].id` value
  - Human Gate: connections for each `approval_options[].id` value

### Flow-Level (Warning)
- Terminal nodes should not have outgoing connections
- Process nodes should have a description or action
- Agent loops should have `max_iterations` set
- Sub-flow `input_mapping` keys should match target flow's contract inputs (if contract defined)
- Sub-flow `output_mapping` keys should match target flow's contract outputs (if contract defined)

### Agent-Specific (Error)
- Agent flow must have at least one `agent_loop` node
- Agent loop must have `model` defined
- Agent loop must have at least one tool
- Agent loop must have at least one terminal tool (`is_terminal: true`)

### Orchestration (Error)
- Orchestrator must have 2+ agents and a strategy
- Smart Router must have rules defined or LLM routing enabled
- Smart Router with empty `fallback_chain` and no LLM routing — warning
- Handoff must have a target (flow or domain)
- Agent Group must have 2+ members

### Extended Nodes (Error)
- Data Store must have operation (and model when store_type is database)
- Service Call must have method and URL
- IPC Call must have command
- Event must have direction and event_name
- Loop must have collection and iterator
- Parallel must have 2+ branches
- Sub-flow must have flow_ref
- LLM Call must have model
- Cache must have key and store
- Transform must have input_schema and output_schema
- Delay must have min_ms
- Collection must have operation, input, and output
- Parse must have format, input, and output
- Crypto must have operation, algorithm, and key_source
- Batch must have input, operation_template, and output
- Transaction must have steps with at least 2 entries

### Domain-Level
- No duplicate flow IDs within a domain
- Multiple flows may publish the same event name with different `from_flow` values — this is valid (dual-purpose events)
- Trigger `event_group:{name}` must reference an `event_groups` entry defined in the same domain's `domain.yaml`
- No duplicate event group names within a domain

### System-Level
- Events consumed by a domain must be published by some domain
- Events published should be consumed by at least one domain (warning)
- Event `payload` fields should match between publisher and consumer across domains (warning)

The DDD Tool validates event wiring across domains automatically (system-level validation). If a domain consumes an event that no domain publishes, the tool flags it as an error. If a domain publishes an event that nothing consumes, it flags a warning.

---

## 10. Workflow: Creating a DDD Project

### Recommended: `/ddd-create` workflow

The fastest way to create a DDD project is with the `/ddd-create` command in Claude Code:

```
/ddd-create A task management app with user auth, projects, tasks with
            assignments and deadlines, and email notifications
```

This generates the complete spec structure — `ddd-project.json`, supplementary specs (including UI specs, infrastructure, schemas with indexes/seed), domain YAML, and flow YAML — covering all four foundational pillars (Logic, Data, Interface, Infrastructure) — ready for visual review.

**Full workflow:**

1. **Create** (Phase 1) — Run `/ddd-create` with a project description. Generates specs for all four pillars: backend flows (Logic), schemas with indexes and seed data (Data), UI page specs (Interface), and infrastructure config (Infrastructure)
2. **Design** (Phase 2) — Open the project in DDD Tool to visualize, validate, and refine specs on the canvas
3. **Scaffold** (Phase 3) — Run `/ddd-scaffold` to set up project skeleton from specs — backend structure, frontend pages, database with seed data, startup scripts
4. **Implement** (Phase 3) — Run `/ddd-implement --all` to generate backend flow code, frontend page components, and tests
5. **Test** (Phase 3) — Run `/ddd-test --all` to verify all tests pass
6. **Reflect** (Phase 4) — Run `/ddd-sync` to check alignment, `/ddd-reflect` to capture implementation wisdom, `/ddd-promote` to move approved patterns into specs
7. **Iterate** — Use `/ddd-status` to check state, `/ddd-update` to modify specs, `/ddd-implement` to update code

> **Note:** Legacy documentation may reference "Session A" (= Phase 1 + Phase 2) and "Session B" (= Phase 3 + Phase 4).

### Alternative: Manual spec creation

You can also create specs by hand:

1. Create `ddd-project.json` with domain list (see Section 2)
2. Create supplementary spec files: `specs/system.yaml`, `specs/architecture.yaml`, `specs/config.yaml`, `specs/shared/errors.yaml`, schema files in `specs/schemas/` (see Section 4)
3. Create domain YAML files: `specs/domains/{domain}/domain.yaml` (see Section 3)
4. Create flow YAML files: `specs/domains/{domain}/flows/{flow}.yaml` (see Section 5)
5. Create UI specs: `specs/ui/pages.yaml` + per-page specs in `specs/ui/` (see Section 4.7)
6. Create infrastructure spec: `specs/infrastructure.yaml` (see Section 4.8)
7. Open in DDD Tool to visualize and validate
8. Run `/ddd-scaffold` to set up project skeleton (backend, frontend, database, startup scripts)
9. Run `/ddd-implement` to generate flow code and page components

---

## 11. Slash Commands

### /ddd-create

Generates a complete DDD project from a natural-language description and/or design files.

**Usage:**
```
/ddd-create <description> [--from <path-or-url>] [--shortfalls]
```

| Flag | Purpose |
|------|---------|
| `--from <path-or-url>` | Use a design file as reference input. Supports images (PNG, JPG), PDFs, markdown, text, YAML, and URLs (Figma, Miro). Extracts domains, flows, data models, UI screens, events, and architecture from the design. |
| `--shortfalls` | Generate `specs/shortfalls.yaml` — a structured gap analysis report documenting DDD framework limitations encountered during design (7 categories: missing node types, inadequate nodes, missing fields, connection limitations, layer gaps, workarounds, cross-cutting gaps). Feed into `/ddd-evolve` for analysis. |

**Examples:**
```
/ddd-create An e-commerce platform with user auth, product catalog,
            shopping cart, order processing, and email notifications

/ddd-create --from ~/designs/wireframes.png Social media dashboard app

/ddd-create --from ~/docs/requirements.pdf AI moderation service. TypeScript, Hono. --shortfalls
```

**What it does:**

`/ddd-create` generates specs for all four foundational pillars of software: **Logic** (backend flows), **Data** (schemas with indexes and seed), **Interface** (UI page specs), and **Infrastructure** (services, ports, startup).

1. Fetches the latest DDD Usage Guide for spec format reference
2. If this is an existing project (`ddd-project.json` exists): reads `architecture.yaml` for `cross_cutting_patterns`, existing domain specs for event wiring, and `.ddd/annotations/` for implementation wisdom. New flows automatically inherit applicable cross-cutting patterns.
3. If `--from` is provided, reads the design file (images, PDFs, markdown, URLs) and extracts domains, flows, data models, UI screens/pages, events, and tech stack
4. Analyzes the description and/or design file. If the description is brief, asks clarifying questions about all four pillars:
   - **Logic**: What are the main domains? Key flows? External services? Agent/AI flows?
   - **Data**: What are the data models? Key relationships? Initial/seed data?
   - **Interface**: What pages/screens? Navigation? Forms? What should the user see and interact with?
   - **Infrastructure**: What services need to run? Which ports? What datastores?
5. Creates the full project structure across all four pillars:
   - **Project config**: `ddd-project.json` with domain list
   - **Logic**: `specs/domains/{domain}/domain.yaml` with flows and event wiring, `specs/domains/{domain}/flows/{flow}.yaml` with full node graphs
   - **Data**: `specs/schemas/` with `_base.yaml` and per-model schemas (including `indexes` and `seed`), `specs/shared/errors.yaml`, `specs/shared/types.yaml`
   - **Interface**: `specs/ui/pages.yaml` (page registry, navigation, theme) + `specs/ui/{page}.yaml` per page (sections, components, data bindings, forms, states)
   - **Infrastructure**: `specs/infrastructure.yaml` (services, ports, startup order, dev commands)
   - **Cross-cutting**: `specs/system.yaml`, `specs/architecture.yaml` (with `cross_cutting_patterns: {}` placeholder), `specs/config.yaml`
6. Produces a **four-pillar extraction table** before generating any files — enumerating all domains/flows (Logic), pages/sections (Interface), schemas (Data), and services (Infrastructure) with counts per pillar. This table is a countable commitment that prevents pillar starvation
7. Generates specs in order: Data → Interface → Infrastructure → Logic (detail-heavy Logic goes last so lighter pillars aren't starved)
8. Validates all flows (trigger → terminals, wired branches, event matching)
9. Validates UI specs (data_source references exist as backend flows, form field types are valid)
10. If `--shortfalls`, generates `specs/shortfalls.yaml` with framework gap analysis
11. Shows summary with per-pillar counts, file list, event wiring, and next steps

**After running:**
1. Open the project in DDD Tool to review visually
2. Refine flows on the canvas if needed
3. Run `/ddd-scaffold` to set up project skeleton (backend, frontend pages, database, startup scripts)
4. Run `/ddd-implement --all` to generate flow code and page components

### /ddd-scaffold

Sets up the project skeleton and shared infrastructure from specs. This is the first step of Phase 3 (Build) — run it before `/ddd-implement`.

**Usage:** `/ddd-scaffold`

**What it does:**
1. Reads all spec files: system.yaml, architecture.yaml, config.yaml, infrastructure.yaml, errors.yaml, types.yaml, schema files, and UI specs
2. Initializes the project (package.json, tsconfig, dependencies, directory structure)
3. **Backend scaffold**: Generates shared infrastructure — config loader, error handler, database schema (with indexes from schemas), app entry point, integration clients, event bus, test setup
4. **Frontend scaffold**: Creates page files and component structure from `specs/ui/pages.yaml` — route definitions, layout components, navigation, shared components. Installs frontend dependencies from pages.yaml config (component library, state management)
5. **Data scaffold**: Generates database seed files from schemas with `seed` sections. Migration strategy seeds run as part of DB setup, fixture strategy seeds are generated as test helpers
6. **Infrastructure scaffold**: Generates startup scripts from `infrastructure.yaml` — `package.json` scripts for dev commands, `docker-compose.yaml` (if production strategy is docker-compose), startup order documentation
7. Generates cross-cutting utility files from `architecture.yaml` → `cross_cutting_patterns`
8. Creates environment files (.env.example, .gitignore)
9. Verifies build compiles and example test passes
10. Initializes `.ddd/mapping.yaml` and `.ddd/annotations/` directory

**After running:** Run `/ddd-implement --all` to generate flow-level code and page components into the scaffolded project.

### /ddd-implement

Generates implementation code from DDD specs.

| Argument | Scope | Example |
|----------|-------|---------|
| `--all` | Entire project | `/ddd-implement --all` |
| `{domain}` | All flows in a domain | `/ddd-implement users` |
| `{domain}/{flow}` | Single flow | `/ddd-implement users/user-register` |
| *(empty)* | Interactive mode | `/ddd-implement` |

**What it does:**
1. Fetches the latest DDD Usage Guide for spec format reference
2. Reads `ddd-project.json` and flow YAML specs
3. Reads supplementary specs (system.yaml, architecture.yaml including `cross_cutting_patterns`, config.yaml, errors.yaml, schemas/, infrastructure.yaml, ui/) for implementation context
4. Checks `.ddd/mapping.yaml` for existing implementations
5. **Backend**: Follows the node graph: trigger → nodes → terminals, implementing all node types (including `collection`, `parse`, `crypto`, `batch`, `transaction`, `cache`, `delay`, `transform`, `ipc_call`)
6. **Frontend**: Reads `specs/ui/{page}.yaml` and generates page components with data fetching (from `data_source` references), forms (from `forms` specs), state management (from `state` config), and loading/error/empty states. Uses `infrastructure.yaml` to configure API client base URLs
7. Applies cross-cutting patterns from `architecture.yaml` to matching nodes — e.g., `stealth_http` to external service calls, `soft_delete` to read operations, `encryption` to credential writes, `api_key_resolution` to flows needing API keys
8. Generates tests (happy path, decision branches, error states, input validation)
9. Runs tests and fixes until passing
10. Updates `.ddd/mapping.yaml` with specHash, file list, and fileHashes

### /ddd-update

Updates DDD project specs (YAML files) to reflect design changes during development. This is the reverse of `/ddd-implement` — instead of code from specs, it updates specs to match new requirements.

| Argument | Scope | Example |
|----------|-------|---------|
| `{domain}/{flow}` | Update a specific flow spec | `/ddd-update users/user-register` |
| `{domain}` | Update domain config and/or its flows | `/ddd-update users` |
| `--add-flow {domain}` | Add a new flow to a domain | `/ddd-update --add-flow users` |
| `--add-domain` | Add a new domain to the project | `/ddd-update --add-domain` |
| *(empty)* | Interactive mode | `/ddd-update` |

**What it does:**
1. Reads current YAML specs
2. Applies the user's requested changes (add/modify/remove nodes, flows, domains)
3. Maintains spec integrity (valid graph, no orphans, proper connections)
4. Preserves unchanged nodes, IDs, and positions
5. Updates `metadata.modified` timestamp
6. Handles cross-domain event wiring impacts
7. Reports what changed and suggests next steps (Reload DDD Tool, re-implement)

**Typical usage in Phase 3 (Build):**
```
User: "Add rate limiting before the login process"
Claude: /ddd-update users/user-login
  → Updates the flow YAML with a new process node
  → "Reload DDD Tool (Cmd+R) to see changes"
  → "Run /ddd-implement users/user-login to update code"
```

### /ddd-sync

Synchronizes specs with implementation state using bidirectional analysis. This is the primary tool for resolving drift safely.

| Argument | What it does |
|----------|-------------|
| *(default)* | Bidirectional sync — classify drift, update hashes for verified flows only |
| `--discover` | Also discover untracked code and propose new specs (analyze → approve → apply) |
| `--fix-drift` | Resolve all drift using the decision tree: metadata→hash, code-ahead→reverse, new-logic→implement |
| `--full` | All of the above |

**Key behavior:** `/ddd-sync` never blindly updates hashes. It classifies each drift (metadata-only, spec enriched, code ahead, new logic) and only updates hashes when both spec→code and code→spec checks pass. Flows where code is ahead of spec are flagged for `/ddd-reverse` instead. See Section 12.1 for the full drift management workflow.

### /ddd-status

Quick read-only overview of the project's implementation state. No files are modified.

**Usage:** `/ddd-status [--json]`

**What it does:**
1. Reads `ddd-project.json`, all domain.yaml files, and `.ddd/mapping.yaml`
2. For each flow, computes status: **Up to date**, **Drifted**, **Stale**, or **Not implemented**
3. **For drifted flows, classifies the drift type** (metadata-only, spec enriched, code ahead, new logic) by reading both the spec diff and the implementation code
4. Checks scaffold state (package.json, entry point, database schema)
5. Displays a table with domain, flow, status (including drift type), and implementation date
6. Suggests next actions using **safe recommendations** — never recommends `/ddd-implement` for drifted flows without confirming the drift type is "new logic". See Section 12.1.

Pass `--json` for machine-readable output.

### /ddd-test

Run tests for implemented flows without re-generating code. Use after manual edits, refactoring, or dependency updates.

**Usage:** `/ddd-test [scope] [--coverage]`

| Argument | Scope | Example |
|----------|-------|---------|
| `--all` | All implemented flows | `/ddd-test --all` |
| `{domain}` | All flows in a domain | `/ddd-test users` |
| `{domain}/{flow}` | Single flow | `/ddd-test users/user-register` |
| *(empty)* | Interactive mode | `/ddd-test` |

**What it does:**
1. Reads `.ddd/mapping.yaml` to find test files for scoped flows
2. Runs the test runner (auto-detected from config files)
3. Reports pass/fail per flow with failure analysis
4. Identifies likely cause: spec drift, manual code change, environment issue, or dependency issue
5. Suggests fix actions per failure

Pass `--coverage` to include coverage reporting.

### /ddd-reverse

Reverse-engineers DDD specs from an existing codebase. Points at source code and generates domain, flow, and schema YAML files.

**Usage:**
```
/ddd-reverse <project-path> [--output <path>] [--domains <d1,d2>] [--merge] [--strategy <name>]
```

| Flag | Purpose | Default |
|------|---------|---------|
| `--output <path>` | Where to write specs | Same as project path |
| `--domains <d1,d2>` | Only reverse specific domains | All domains |
| `--merge` | Merge with existing specs (don't overwrite) | Overwrite |
| `--strategy <name>` | Override auto-selected strategy | Auto by file count |

**Strategies** (auto-selected based on source file count):

| Strategy | Files | Approach |
|----------|-------|---------|
| `baseline` | < 30 | Read code directly in context |
| `index` | 30–80 | Build in-context index, process per-domain |
| `swap` | 80–150 | Write index to `.ddd/reverse/` on disk, read selectively |
| `bottom-up` | 150–300 | Grep entry points (L3), extract each independently, group into domains (L2→L1) |
| `compiler` | 300–500 | 6-pass pipeline: scan → extract → resolve → IR → link → emit |
| `codex` | 500+ | Compress codebase to ref code vocabulary + one-line call chains |

**Examples:**
```
/ddd-reverse ~/code/my-api
/ddd-reverse ~/code/my-app --output ~/specs --domains users,orders
/ddd-reverse ~/code/my-app --merge --strategy compiler
```

**What it does:**
1. Detects tech stack from config files (package.json, Cargo.toml, go.mod, etc.)
2. Scans project structure, infers domain boundaries
3. Extracts data models from ORM/schema definitions
4. Extracts flows from routes, handlers, event listeners, cron jobs
5. Extracts cross-cutting concerns (errors, shared types, config, architecture)
6. Wires events across domains
7. Runs quality checks and coverage verification
8. Generates draft YAML specs in `specs/` and coverage report at `.ddd/reverse/coverage.yaml`

### /ddd-evolve

Analyzes DDD shortfall reports from one or more projects, critically evaluates each gap, and produces a prioritized evolution plan for human decision-making. This is how DDD improves itself based on real project feedback.

**Usage:**
```
/ddd-evolve <shortfalls.yaml> [<shortfalls2.yaml> ...]
/ddd-evolve --dir <project-dir> [--dir <project-dir2> ...]
/ddd-evolve --review <evolution-plan.yaml>
/ddd-evolve --apply <evolution-plan.yaml>
```

| Mode | What it does |
|------|-------------|
| *(default)* | Analyze shortfalls → produce `ddd-evolution-plan.yaml` with recommendations |
| `--review` | Walk through each item interactively, collect approve/defer/reject decisions |
| `--apply` | Execute approved items from a reviewed plan (requires `--review` first) |

| Flag | Purpose |
|------|---------|
| `--dir <path>` | Point to a DDD project directory. Auto-discovers `specs/shortfalls.yaml` inside it. Verifies `ddd-project.json` exists. Can be specified multiple times. Can be mixed with direct file paths. |

**What it does:**
1. Reads shortfall files (generated by `/ddd-create --shortfalls`) and deduplicates across projects
2. Critically evaluates each shortfall through 6 filters:
   - **Already possible?** — check if DDD can already do it
   - **Frequency test** — 1 project = weak signal, 3+ = systemic
   - **Specificity test** — actionable or vague?
   - **Scope test** — breaking vs additive vs docs-only
   - **Workaround quality** — blocked vs lossy vs adequate
   - **Design intent** — intentional limitation or real gap?
3. Classifies each as: `REAL_GAP`, `ENHANCEMENT`, `VAGUE`, `ALREADY_POSSIBLE`, `BY_DESIGN`, or `PROJECT_SPECIFIC`
4. Produces a tiered evolution plan. Nothing changes until a human reviews and approves.

**Evolve DDD workflow:**
```
/ddd-create projA --shortfalls  →  specs/shortfalls.yaml   ┐
/ddd-create projB --shortfalls  →  specs/shortfalls.yaml   ├→ /ddd-evolve → --review → --apply
/ddd-create projC --shortfalls  →  specs/shortfalls.yaml   ┘
```

### /ddd-reflect

Captures implementation wisdom — patterns and details that code has but specs don't describe. Creates annotation files for human review.

**Usage:**
```
/ddd-reflect [scope]
```

| Argument | Scope | Example |
|----------|-------|---------|
| `--all` | Entire project | `/ddd-reflect --all` |
| `{domain}` | All flows in a domain | `/ddd-reflect monitoring` |
| `{domain}/{flow}` | Single flow | `/ddd-reflect monitoring/check-social-sources` |
| *(empty)* | Interactive mode | `/ddd-reflect` |

**What it does:**
1. Reads the flow spec YAML and the implementation files from mapping.yaml
2. Compares: what does code do that spec doesn't describe?
3. Classifies findings into pattern categories (stealth_http, encryption, soft_delete, api_key_resolution, content_hashing, error_handling, custom) using `architecture.yaml` cross_cutting_patterns as reference
4. Checks if findings match existing annotations (skips duplicates)
5. Writes annotations to `.ddd/annotations/{domain}/{flow}.yaml`
6. Updates mapping.yaml `annotationCount` for each flow
7. Reports summary: N patterns found, M new annotations created, K already captured

### /ddd-promote

Moves approved annotations into permanent specs. This is how implementation wisdom becomes part of the design.

**Usage:**
```
/ddd-promote [scope]
```

| Argument | Scope | Example |
|----------|-------|---------|
| `--all` | Promote all approved annotations | `/ddd-promote --all` |
| `--review` | Interactive review of candidates | `/ddd-promote --review` |
| `{domain}/{flow}` | Scope to specific flow | `/ddd-promote monitoring/check-social-sources` |

**What it does:**
1. Reads all `.ddd/annotations/` files
2. Groups by status: candidate, approved, dismissed
3. Presents candidates to user with code evidence
4. User approves/dismisses each
5. For approved patterns:
   - Cross-cutting convention → add/update `architecture.yaml` cross_cutting_patterns
   - Flow-specific detail → enrich the flow spec YAML
   - Shared type/error → update shared/types.yaml or shared/errors.yaml
6. Updates annotation status to "promoted" or "dismissed"
7. Updates mapping.yaml specHash (spec files changed)
8. Reports what was promoted and where

### Shortfalls & Evolve: End-to-End Example

The `--shortfalls` flag on `/ddd-create` and the `/ddd-evolve` command form a feedback loop that lets DDD improve itself based on real project experience. Here's a complete walkthrough.

#### Step 1: Generate specs with shortfall tracking

```
/ddd-create A real-time collaborative whiteboard with users, boards, shapes,
            and presence tracking. TypeScript, Hono, PostgreSQL, Redis. --shortfalls
```

While creating specs, Claude tracks every framework limitation it hits — places where a structured node type doesn't exist, where fields are missing, where a workaround was needed. These get written to `specs/shortfalls.yaml`.

#### Step 2: Review the shortfall report

The generated `specs/shortfalls.yaml` has 7 categories:

```yaml
# specs/shortfalls.yaml
project: collaborative-whiteboard
generated: "2026-02-19T10:00:00Z"
ddd_version: "1.0"

missing_node_types:
  - name: "websocket_broadcast"
    severity: high
    description: "Broadcast a message to all connected WebSocket clients in a room"
    used_instead: "process node with free-text description"
    flows_affected:
      - "boards/shape-update"
      - "boards/cursor-move"
    example_use_case: "When a user moves a shape, broadcast the new position to all
                       other users viewing the same board in real-time"

inadequate_existing_nodes:
  - node_type: "data_store"
    severity: medium
    limitation: "No support for Redis pub/sub operations — only CRUD"
    suggestion: "Add store_type: 'pubsub' with operations: publish, subscribe, unsubscribe"
    flows_affected:
      - "presence/track-cursor"

missing_spec_fields:
  - location: "trigger"
    field_name: "connection_lifecycle"
    severity: high
    description: "WebSocket triggers need to express on_connect, on_message, and
                  on_disconnect as separate entry points within the same flow"
    workaround: "Created three separate flows instead of one"

connection_limitations:
  - severity: medium
    limitation: "Cannot express a bidirectional data channel between two nodes"
    context: "Shape sync requires sending updates to the client while also
              receiving updates — current connections are unidirectional"
    suggestion: "Add a 'bidirectional' flag on connections or a 'channel' node type"

workarounds:
  - flow: "boards/shape-update"
    node_id: "process-kR9mX2vL"
    node_type_used: "process"
    intended_operation: "Broadcast shape delta to all WebSocket clients in the room"
    why_no_fit: "No node type handles WebSocket fan-out to multiple clients"
    proposed_node_type: "websocket_broadcast or enhance event node with
                         delivery: 'websocket_room'"
    severity: high

cross_cutting_gaps:
  - concern: "rate_limiting"
    severity: medium
    description: "Shape move events fire at 60fps — need per-client throttling
                  before processing"
    current_expression: "Described in process node free-text"
    suggestion: "Add a rate_limit field on trigger nodes with window_ms and max_count"

summary:
  total_shortfalls: 8
  by_severity:
    critical: 0
    high: 3
    medium: 4
    low: 1
  top_recommendation: "Add WebSocket/real-time primitives — ws triggers lack
                       lifecycle hooks and there's no broadcast node type"
```

The report is project-specific. Each entry references actual flows and nodes from the design.

#### Step 3: Accumulate shortfalls across projects

Shortfalls from a single project are weak signal. Run multiple projects with `--shortfalls` to see patterns:

```
/ddd-create projA --shortfalls    # → projA/specs/shortfalls.yaml
/ddd-create projB --shortfalls    # → projB/specs/shortfalls.yaml
/ddd-create projC --shortfalls    # → projC/specs/shortfalls.yaml
```

#### Step 4: Analyze with /ddd-evolve

Feed all shortfall files into `/ddd-evolve`:

```
/ddd-evolve --dir ~/code/projA --dir ~/code/projB --dir ~/code/projC
```

Or pass files directly:

```
/ddd-evolve ~/code/projA/specs/shortfalls.yaml ~/code/projB/specs/shortfalls.yaml
```

`/ddd-evolve` deduplicates across projects, evaluates each shortfall through 6 filters (already possible? recurring? specific? breaking? adequate workaround? intentional?), and classifies each as:

| Classification | Meaning |
|----------------|---------|
| `REAL_GAP` | Genuine framework limitation, seen across projects |
| `ENHANCEMENT` | Nice-to-have improvement, not blocking |
| `VAGUE` | Too vague to act on — needs more data |
| `ALREADY_POSSIBLE` | DDD already supports this, shortfall was a misunderstanding |
| `BY_DESIGN` | Intentional limitation — fixing would break the model |
| `PROJECT_SPECIFIC` | Only relevant to one project, not a systemic gap |

Output: `ddd-evolution-plan.yaml` with tiered recommendations.

#### Step 5: Interactive review

```
/ddd-evolve --review ddd-evolution-plan.yaml
```

Walk through each item. For each shortfall, `/ddd-evolve` presents its analysis and evidence. You decide: **approve**, **defer**, or **reject**. Decisions are recorded back into the plan file.

#### Step 6: Apply approved changes

```
/ddd-evolve --apply ddd-evolution-plan.yaml
```

Executes only the approved items — updates spec templates, commands, tool validator, or documentation. Requires a reviewed plan (won't run on unreviewed items).

#### The 7 Shortfall Categories

| Category | What it captures | Example |
|----------|-----------------|---------|
| `missing_node_types` | Node concepts that don't exist at all | WebSocket broadcast, file watcher |
| `inadequate_existing_nodes` | Existing nodes that lack needed capabilities | data_store missing pub/sub ops |
| `missing_spec_fields` | Fields that should exist on nodes/flows/triggers | Trigger missing lifecycle hooks |
| `connection_limitations` | Edge behaviors that can't be expressed | Bidirectional data channels |
| `layer_gaps` | Information invisible at L1/L2/L3 layers | Cross-domain latency not shown on system map |
| `workarounds` | Every `process` node used because no structured type fit | Broadcast described in free-text |
| `cross_cutting_gaps` | Patterns spanning multiple flows with no structural support | Rate limiting, feature flags |

#### Severity Levels

| Level | Meaning |
|-------|---------|
| `critical` | Blocked design intent — can't express the system correctly |
| `high` | Significant workaround needed — lossy or fragile |
| `medium` | Minor workaround — functional but inelegant |
| `low` | Cosmetic or nice-to-have |

---

## 12. mapping.yaml

**Path:** `.ddd/mapping.yaml`

Tracks what has been implemented:

```yaml
flows:
  users/user-register:
    spec: specs/domains/users/flows/user-register.yaml  # Path to the flow spec file
    specHash: a1b2c3d4e5f6...    # SHA-256 of the flow YAML
    implementedAt: "2025-01-15T14:30:00Z"
    mode: new                     # new | update — whether first implementation or re-implementation
    files:
      - src/routes/auth.ts
      - src/services/user-service.ts
      - src/models/user.ts
      - tests/routes/auth.test.ts
    fileHashes:                    # Per-file hashes for reverse drift detection
      src/routes/auth.ts: abc123...
      src/services/user-service.ts: def456...
    syncState: in_sync              # in_sync | spec_ahead | code_ahead | diverged | new_logic
    annotationCount: 0             # Number of pending annotations from /ddd-reflect
  users/user-login:
    spec: specs/domains/users/flows/user-login.yaml
    specHash: f6e5d4c3b2a1...
    implementedAt: "2025-01-16T10:00:00Z"
    mode: update
    files:
      - src/routes/auth.ts
      - src/services/auth-service.ts
    syncState: spec_ahead
    annotationCount: 2

pages:
  dashboard:
    spec: specs/ui/dashboard.yaml
    specHash: <SHA-256 of spec content>
    implementedAt: "2025-01-15T10:30:00Z"
    mode: new
    files:
      - src/pages/dashboard.tsx
      - src/components/dashboard/StatsCard.tsx
    fileHashes:
      src/pages/dashboard.tsx: ghi789...
    syncState: in_sync
    annotationCount: 0
```

**Drift detection:** If the specHash no longer matches the flow YAML content, the flow has "drifted" and needs resolution — but NOT necessarily re-implementation. See Section 12.1 for the correct drift resolution workflow.

### 12.1 Drift Management Workflow

When `specHash` doesn't match the current flow YAML, it means specs and code have diverged. **The critical question is: in which direction?**

#### Drift Types

| Drift Type | What Happened | How to Detect |
|------------|---------------|---------------|
| **Metadata-only** | DDD Tool saved new timestamps, positions, or formatting. No logic changed. | Diff the YAML — only `metadata.*`, `position`, or whitespace fields differ |
| **Spec enriched** | Spec added detail (descriptions, field types) but code already handles it. | Read the code — it already does what the updated spec describes |
| **Code ahead** | Implementation has patterns (error handling, caching, stealth HTTP, encryption) the spec doesn't describe. | Read the code — it does MORE than the spec describes |
| **New spec logic** | Nodes, connections, tools, or business logic were added to the spec that code doesn't implement. | Read the code — it does LESS than what the updated spec describes |

#### Resolution Decision Tree

```
Hash mismatch detected
  │
  ├─ Metadata-only?
  │   └─ YES → /ddd-sync (updates hash, no code change)
  │
  ├─ Code already covers the spec change?
  │   └─ YES → /ddd-sync (updates hash after verification)
  │
  ├─ Code has details spec doesn't describe?
  │   └─ YES → /ddd-reverse {domain/flow} first to enrich specs,
  │            then /ddd-sync to align hashes
  │
  └─ Spec has new logic code doesn't implement?
      └─ YES → /ddd-implement {domain/flow}
               (ONLY case where re-implementation is correct)
```

#### Critical Rules

1. **Never run `/ddd-implement` without classifying the drift first.** Re-implementing a flow overwrites existing code. If the code has implementation details the spec doesn't capture (stealth HTTP, encryption, error handling patterns, etc.), those details will be lost.

2. **Never manually update `specHash` in mapping.yaml.** This falsely declares "in sync" without verifying both directions. Use `/ddd-sync` which performs proper bidirectional analysis.

3. **Never update a hash when code is ahead of spec.** If implementation has details the spec doesn't describe, the correct action is `/ddd-reverse` to enrich the spec first, then `/ddd-sync` to update the hash. Updating the hash directly means future `/ddd-implement` runs will generate code WITHOUT those details.

4. **Cross-cutting patterns belong in `architecture.yaml`.** Implementation details used across multiple flows (stealth HTTP, encryption, API key resolution with DB+env fallback, soft-delete conventions) should be documented in `architecture.yaml` so all flows benefit from them during implementation.

#### Bidirectional Sync Check

Before any hash update, verify BOTH directions:

| Check | Question | If NO |
|-------|----------|-------|
| **Spec → Code** | Does the code implement everything in the current spec? | Need `/ddd-implement` |
| **Code → Spec** | Does the spec describe everything the code does? | Need `/ddd-reverse` first |

Only update the hash when BOTH answers are YES.

### 12.2 Annotations Schema

#### Annotation Paths

Annotations are organized by pillar:

- **Logic (flows):** `.ddd/annotations/{domain}/{flow}.yaml`
- **Interface (pages):** `.ddd/annotations/ui/{page-id}.yaml`
- **Data (schemas):** `.ddd/annotations/schemas/{model}.yaml`
- **Infrastructure:** `.ddd/annotations/infrastructure.yaml`

Annotations capture implementation wisdom — patterns and details that code has but specs don't describe. They are created by `/ddd-reflect` and promoted into permanent specs by `/ddd-promote`.

```yaml
flow: domain/flow-id
captured_at: "2025-01-20T10:00:00Z"
captured_from: reflect           # reflect | reverse | sync
patterns:
  - id: pattern-id
    type: stealth_http           # stealth_http | api_key_resolution | encryption | soft_delete | content_hashing | error_handling | custom
    description: >
      What the code does that spec doesn't capture
    applies_to_nodes: [node-id-1, node-id-2]
    code_evidence:
      file: src/path/to/file.ts
      lines: "45-67"
      snippet: "brief code excerpt"
    status: candidate            # candidate | approved | promoted | dismissed
implementation_details:
  - node_id: node-id
    detail: what the code adds beyond the spec
    code_evidence:
      file: src/path/to/file.ts
      lines: "12-30"
```

**Pattern types:**

| Type | What it captures |
|------|-----------------|
| `stealth_http` | User-agent rotation, proxy pools, cookie jars, headless browser fallback |
| `api_key_resolution` | Multi-source key lookup (DB → env fallback), key validation |
| `encryption` | Field-level encryption, key management, algorithm choices |
| `soft_delete` | `deletedAt: null` filters on all reads, cascade behavior |
| `content_hashing` | Deduplication via content hashes, hash algorithm choices |
| `error_handling` | Retry patterns, circuit breakers, graceful degradation |
| `custom` | Project-specific patterns not in the standard categories |

**Pattern types by pillar:**

- **Logic (flows):** `stealth_http | api_key_resolution | encryption | soft_delete | content_hashing | error_handling | custom`
- **Interface (pages):** `component_composition | data_fetching | state_management | form_validation | responsive_layout | accessibility | animation | error_handling | custom`
- **Data (schemas):** `index_optimization | migration_pattern | seed_data | constraint | query_optimization | custom`
- **Infrastructure:** `docker_config | startup_orchestration | dev_tooling | ci_cd | resource_limits | custom`

**Status lifecycle:**
```
candidate → approved → promoted    (into specs)
candidate → dismissed              (not useful)
```

### 12.3 Cross-Cutting Patterns in architecture.yaml

The `cross_cutting_patterns` section in `architecture.yaml` documents project-specific conventions discovered during the Build and Reflect phases. Each pattern describes a recurring implementation approach that all matching flows should follow. `/ddd-implement` reads these patterns and applies them automatically.

```yaml
cross_cutting_patterns:
  stealth_http:
    description: >
      Rotate user agents, use proxy pools, maintain cookie jars per domain,
      add random delays between requests to avoid bot detection
    utility: src/utils/stealth-http.ts
    config:
      user_agent_pool: 50
      proxy_rotation: per_request
      cookie_jar: per_domain
    used_by_domains: [monitoring, discovery]
    convention: >
      All service_call nodes that fetch external content in monitoring and
      discovery domains must use the stealth HTTP utility instead of raw fetch

  api_key_resolution:
    description: >
      Resolve API keys from database first (user's own keys), fall back to
      environment variables (system keys). Validate key format before use.
    utility: src/utils/api-keys.ts
    config:
      sources: [database, environment]
      validation: true
    used_by_domains: [settings, monitoring, discovery, publishing]
    convention: >
      Any flow that needs an external API key must use the api-key resolution
      utility. Never hardcode keys or read only from env.

  encryption:
    description: >
      AES-256-GCM encryption for sensitive fields (API keys, tokens, credentials).
      Key derived from ENCRYPTION_KEY env var.
    utility: src/utils/encryption.ts
    config:
      algorithm: aes-256-gcm
      key_env: ENCRYPTION_KEY
    used_by_domains: [settings]
    convention: >
      All credential fields stored in the database must be encrypted at rest.
      Decrypt on read, encrypt on write.
```

---

## 13. Flow Templates

When creating a flow in the DDD Tool, these templates are available:

### Traditional Templates
| Template | Nodes | Pattern |
|----------|-------|---------|
| REST API Endpoint | 5 | Trigger -> Input --(valid)--> Process -> Terminal (success) / --(invalid)--> Terminal (error) |
| CRUD Entity | 6 | Trigger -> Input --(valid)--> Decision --(true)--> Data Store -> Terminal (success) / --(false)--> Terminal (forbidden) |
| Webhook Handler | 6 | Trigger -> Input --(valid)--> Process -> Service Call --(success)--> Terminal / --(error)--> Terminal (error) |
| Event Processor | 5 | Trigger (`event:`) -> Process -> Event (emit) -> Terminal (success) / Terminal (error) |
| Cached API Call | 7 | Trigger -> Input -> Cache --(miss)--> Service Call -> Transform -> Terminal / --(hit)--> Transform -> Terminal |
| Collection Processing | 6 | Trigger -> Collection (filter) --(result)--> Loop --(body)--> Event (emit) / --(done)--> Terminal / --(empty)--> Terminal |
| Data Import with Parsing | 7 | Trigger -> Service Call --(success)--> Parse --(success)--> Collection (deduplicate) -> Data Store -> Terminal / --(error)--> Terminal |

### Agent Templates
| Template | Nodes | Pattern |
|----------|-------|---------|
| RAG Agent | 6 | Trigger -> Guardrail --(pass)--> Agent Loop --(done)--> Guardrail -> Terminal / --(error)--> Terminal |
| Customer Support Agent | 7 | Trigger -> Guardrail --(pass)--> Agent Loop --(done)--> Human Gate --(approve)--> Terminal / --(reject)--> Terminal / --(error)--> Terminal |
| Code Review Agent | 4 | Trigger -> Agent Loop (analysis) --(done)--> Terminal / --(error)--> Terminal |
| Data Pipeline Agent | 4 | Trigger -> Agent Loop (ETL) --(done)--> Terminal / --(error)--> Terminal |

---

## 14. Complete Example: E-Commerce Project

### ddd-project.json

```json
{
  "domains": [
    { "name": "Users", "description": "Authentication and user profiles" },
    { "name": "Products", "description": "Product catalog management" },
    { "name": "Orders", "description": "Order processing and fulfillment" },
    { "name": "Notifications", "description": "Email and push notifications" }
  ]
}
```

### specs/system.yaml

```yaml
name: ecommerce-api
version: "1.0.0"
description: E-commerce platform API

tech_stack:
  language: TypeScript
  runtime: Node.js 20
  framework: Express 4
  database: PostgreSQL 16
  orm: Prisma
  cache: Redis 7
  queue: BullMQ
  auth: JWT + bcrypt
```

> **errors.yaml** and **schemas** follow the same format as Sections 4.4 and 4.5. Project-specific errors (e.g., `INSUFFICIENT_STOCK`) are added alongside the standard set.

### specs/schemas/order.yaml

```yaml
name: Order
description: Customer order
inherits: _base
fields:
  - name: user_id
    type: uuid
    required: true
  - name: items
    type: json
    required: true
  - name: shipping_address
    type: json
    required: true
  - name: status
    type: enum
    values: [pending, confirmed, shipped, delivered, cancelled]
    default: pending
  - name: total
    type: decimal
    required: true
indexes:
  - fields: [user_id, status]
relationships:
  - name: user
    type: belongs_to
    target: User
    foreign_key: user_id
```

### specs/domains/users/domain.yaml

```yaml
name: Users
description: Authentication and user profiles
flows:
  - id: user-register
    name: User Registration
    type: traditional
  - id: user-login
    name: User Login
    type: traditional
  - id: reset-password
    name: Password Reset
    type: traditional
publishes_events:
  - event: UserRegistered
    from_flow: user-register
    description: New user created
  - event: PasswordResetRequested
    from_flow: reset-password
consumes_events: []
layout:
  flows:
    user-register: { x: 100, y: 100 }
    user-login: { x: 100, y: 300 }
    reset-password: { x: 100, y: 500 }
  portals: {}
```

### specs/domains/orders/domain.yaml

```yaml
name: Orders
description: Order processing and fulfillment
flows:
  - id: create-order
    name: Create Order
    type: traditional
  - id: process-payment
    name: Process Payment
    type: traditional
publishes_events:
  - event: OrderCreated
    from_flow: create-order
  - event: PaymentProcessed
    from_flow: process-payment
  - event: PaymentFailed
    from_flow: process-payment
consumes_events:
  - event: UserRegistered
    handled_by_flow: create-order
    description: Pre-populate user data for new orders
layout:
  flows:
    create-order: { x: 100, y: 100 }
    process-payment: { x: 100, y: 300 }
  portals: {}
```

### specs/domains/orders/flows/create-order.yaml (key nodes only)

> Full flow follows the same structure as Section 5's user-register example. Shown here are the novel patterns: `service_call` with `error_mapping`, `event` emit, and multiple error terminals.

```yaml
  # service_call with error_mapping — maps HTTP errors to error codes from errors.yaml
  - id: sc-001
    type: service_call
    connections:
      - targetNodeId: decision-001
        sourceHandle: "success"
      - targetNodeId: terminal-inventory-error
        sourceHandle: "error"
    spec:
      method: POST
      url: "https://inventory.internal/api/reserve"
      body:
        order_id: "$.order.id"
        items: "$.items"
      timeout_ms: 5000
      retry:
        max_attempts: 3
        backoff_ms: 1000
      error_mapping:
        "409": "INSUFFICIENT_STOCK"
        "503": "SERVICE_UNAVAILABLE"
    label: Reserve Inventory

  # event emit — async cross-domain notification
  - id: event-001
    type: event
    connections:
      - targetNodeId: terminal-success
    spec:
      direction: emit
      event_name: OrderCreated
      payload:
        order_id: "$.order.id"
        user_id: "$.user_id"
        total: "$.order.total"
      async: true
    label: Emit OrderCreated

  # error terminals — each maps to an error code from errors.yaml
  - id: terminal-inventory-error
    type: terminal
    spec:
      outcome: error
      status: 503
      body:
        error: "SERVICE_UNAVAILABLE"
    label: Inventory Service Error
```

### Agent Flow Example: specs/domains/support/flows/support-ticket.yaml

```yaml
flow:
  id: support-ticket
  name: Support Ticket Handler
  type: agent
  domain: support
  description: AI agent that handles customer support tickets

trigger:
  id: trigger-001
  type: trigger
  position: { x: 250, y: 50 }
  connections:
    - targetNodeId: guard-input
  spec:
    event: "event:SupportTicketCreated"
    source: Ticketing System
    description: New support ticket arrives
  label: New Ticket

nodes:
  - id: guard-input
    type: guardrail
    position: { x: 250, y: 170 }
    connections:
      - targetNodeId: agent-001
        sourceHandle: "pass"
      - targetNodeId: terminal-blocked
        sourceHandle: "block"
    spec:
      position: input
      checks:
        - type: content_policy
          action: block
        - type: prompt_injection
          action: block
      on_block: Reject with policy violation message
    label: Input Guard

  - id: agent-001
    type: agent_loop
    position: { x: 250, y: 320 }
    connections:
      - targetNodeId: gate-001
        sourceHandle: "done"
      - targetNodeId: terminal-agent-error
        sourceHandle: "error"
    spec:
      model: claude-sonnet
      system_prompt: >
        You are a customer support agent. Investigate the issue,
        search the knowledge base, and propose a resolution.
      max_iterations: 8
      temperature: 0.5
      stop_conditions:
        - resolution_proposed
        - escalation_needed
      tools:
        - id: lookup
          name: lookup_ticket
          description: Look up an existing support ticket
          parameters: '{"ticket_id": "string"}'
        - id: search
          name: search_knowledge_base
          description: Search internal KB for solutions
          parameters: '{"query": "string"}'
        - id: resolve
          name: propose_resolution
          description: Propose a resolution for human review
          parameters: '{"resolution": "string"}'
          is_terminal: true
          requires_confirmation: true
      memory:
        - name: conversation
          type: conversation_history
          max_tokens: 8000
          strategy: sliding_window
      on_max_iterations: escalate
    label: Support Agent

  - id: gate-001
    type: human_gate
    position: { x: 250, y: 480 }
    connections:
      - targetNodeId: terminal-001
        sourceHandle: "approve"
      - targetNodeId: terminal-rejected
        sourceHandle: "reject"
    spec:
      notification_channels:
        - slack
        - email
      approval_options:
        - id: approve
          label: Approve Resolution
        - id: reject
          label: Reject & Reassign
          requires_input: true
      timeout:
        duration: 1800
        action: escalate
      context_for_human:
        - ticket_summary
        - proposed_resolution
        - customer_history
    label: Escalation Gate

  - id: terminal-001
    type: terminal
    position: { x: 250, y: 600 }
    connections: []
    spec:
      outcome: ticket resolved
      description: Ticket has been resolved
    label: Resolved

  - id: terminal-agent-error
    type: terminal
    position: { x: 450, y: 420 }
    connections: []
    spec:
      outcome: agent error
      description: Agent loop failed or hit max iterations
    label: Agent Error

  - id: terminal-rejected
    type: terminal
    position: { x: 450, y: 580 }
    connections: []
    spec:
      outcome: rejected
      description: Human reviewer rejected and reassigned
    label: Rejected & Reassigned

  - id: terminal-blocked
    type: terminal
    position: { x: 450, y: 270 }
    connections: []
    spec:
      outcome: policy violation
      description: Input blocked by guardrail
    label: Blocked

metadata:
  created: "2025-01-10T10:00:00Z"
  modified: "2025-01-10T10:00:00Z"
```

---

## 15. Tips for Writing Good Specs

1. **Use descriptive labels** — node labels appear on the canvas and in generated code comments
2. **Fill in all spec fields** — empty specs trigger validation warnings
3. **Connect everything** — no orphaned nodes, all paths must reach a terminal
4. **Use consistent event names** — stick to PascalCase (e.g., `UserRegistered`, `OrderCreated`)
5. **Decision nodes need both branches** — always wire both `true` and `false` handles
6. **Agent flows need terminal tools** — at least one tool must have `is_terminal: true`
7. **Use `flow_ref` format** — sub_flow references should be `domain-id/flow-id`
8. **Node IDs must be unique** within a flow — use a prefix matching the type (e.g., `input-001`, `process-002`)
9. **Position doesn't affect logic** — positions are for canvas layout only, connections define the actual flow
10. **Cross-cutting concerns are optional** — only add observability/security when needed for implementation hints
11. **Always use sourceHandle on branching nodes** — see Section 8 for the complete handle reference per node type
12. **Create supplementary specs early** — system.yaml, architecture.yaml, config.yaml, errors.yaml, and schemas give `/ddd-implement` the context it needs to generate correct code
13. **Use trigger conventions** — prefix with `HTTP`, `cron`, `event:`, `webhook`, `manual`, `sse`, `ws`, `pattern:`, `shortcut`, `timer`, `ui:`, or `ipc:` to communicate trigger type
14. **Add status and body to terminals** — custom fields on terminal specs tell `/ddd-implement` exactly what HTTP response to generate
15. **Define integrations in system.yaml** — `service_call` nodes reference integration names instead of repeating URL/auth/retry config
16. **Use shared/types.yaml for enums used across schemas** — avoids duplicating `values:` arrays in multiple schema files
17. **Add state transitions to schemas with lifecycle fields** — makes valid state changes explicit and `/ddd-implement` generates validation logic
18. **Use smart_router for 3+ way branching** — it works in traditional flows too, not just agent flows
19. **Use `collection` nodes for filter/sort/deduplicate** — instead of process nodes with vague descriptions like "Filter items" or "Merge and deduplicate results"
20. **Use `parse` nodes for RSS/HTML/XML extraction** — instead of process workarounds like "Parse RSS entries" or "Extract with cheerio"
21. **Use `crypto` nodes for encryption/hashing** — instead of free-text process descriptions like "Encrypt key value with AES-256"
22. **Use trigger `filter` to avoid unnecessary decision nodes** — if you only need to check event payload fields, add a `filter` on the trigger instead of a decision node after it
23. **Use `accumulate` on loops to collect results** — instead of relying on implicit variables, use the `accumulate` field to formally collect and name iteration results
24. **Use `include` on data_store for joins** — instead of multiple sequential data_store reads, use the `include` field to eager-load related records in a single operation

---

## 16. Design Patterns

Common patterns and conventions for DDD Tool specs.

### HTTP Request/Response Approval

For synchronous approval flows (e.g., "admin approves a pending request"), use an HTTP trigger + decision node — **not** a human_gate. Human gates are for async agent workflows where an AI agent pauses and waits for human review.

```yaml
# Correct: HTTP approval flow
trigger:
  spec:
    event: "HTTP POST /api/requests/{id}/approve"
    source: API Gateway

nodes:
  - type: decision
    spec:
      condition: "user.role === 'admin'"
      trueLabel: Authorized
      falseLabel: Forbidden
    connections:
      - targetNodeId: process-approve
        sourceHandle: "true"
      - targetNodeId: terminal-forbidden
        sourceHandle: "false"
```

### Guardrail Execution Model

Guardrails are **inline and sequential** — they sit in the connection chain and data flows through them in order. They are NOT sidecars or parallel watchers.

```
trigger -> guardrail(input) -> agent_loop -> guardrail(output) -> terminal
```

The input guardrail validates/filters data before it reaches the agent. The output guardrail validates the agent's response before it reaches the user. If a guardrail blocks, the flow stops at that point (see `on_block`).

### Multi-Way Routing in Traditional Flows

Decision nodes are binary (true/false). When you need 3+ branches, use `smart_router` — it works in traditional flows too, not just agent flows:

```yaml
# Use smart_router for multi-way branching in traditional flows
- id: router-001
  type: smart_router
  connections:
    - targetNodeId: handle-twitter
      sourceHandle: "twitter"
    - targetNodeId: handle-linkedin
      sourceHandle: "linkedin"
    - targetNodeId: handle-medium
      sourceHandle: "medium"
  spec:
    rules:
      - id: twitter
        condition: "platform === 'twitter'"
        route: twitter
      - id: linkedin
        condition: "platform === 'linkedin'"
        route: linkedin
      - id: medium
        condition: "platform === 'medium'"
        route: medium
    fallback_chain: [medium]
```

> **Note:** `smart_router` is listed under Orchestration Nodes but can be used in traditional flows for multi-way routing. Use it instead of chaining decision nodes when you have 3+ branches.

### Collection Pipeline

Process a collection through multiple stages: fetch → parse → filter → iterate → reduce → emit.

```yaml
# Fetch raw data
- id: sc-001
  type: service_call
  connections:
    - targetNodeId: parse-001
      sourceHandle: "success"
  spec:
    method: GET
    url: "https://api.example.com/feed"

# Parse raw response
- id: parse-001
  type: parse
  connections:
    - targetNodeId: coll-filter
      sourceHandle: "success"
  spec:
    format: rss
    input: "$.raw_response"
    output: "entries"

# Filter relevant entries
- id: coll-filter
  type: collection
  connections:
    - targetNodeId: loop-001
      sourceHandle: "result"
    - targetNodeId: terminal-empty
      sourceHandle: "empty"
  spec:
    operation: filter
    input: "$.entries"
    predicate: "item.published_at > $.last_checked_at"
    output: "new_entries"

# Iterate and process each entry
- id: loop-001
  type: loop
  connections:
    - targetNodeId: process-item
      sourceHandle: "body"
    - targetNodeId: coll-reduce
      sourceHandle: "done"
  spec:
    collection: "$.new_entries"
    iterator: entry
    accumulate:
      field: processed_items
      strategy: append
      output: "all_processed"

# Reduce results
- id: coll-reduce
  type: collection
  connections:
    - targetNodeId: event-001
      sourceHandle: "result"
  spec:
    operation: deduplicate
    input: "$.all_processed"
    key: "item.url"
    output: "unique_items"

# Emit event with results
- id: event-001
  type: event
  connections:
    - targetNodeId: terminal-done
  spec:
    direction: emit
    event_name: NewContentDiscovered
    payload_source: "$.unique_items"
```

### Parameterized Flow

Define a flow template once, instantiate per-configuration in domain.yaml. Eliminates near-duplicate flows.

```yaml
# specs/domains/publishing/flows/publish-to-platform.yaml
flow:
  id: publish-to-platform
  name: Publish to Platform
  type: traditional
  domain: publishing
  template: true
  parameters:
    platform:
      type: string
      values: [twitter, linkedin, facebook]
    api_integration:
      type: integration_ref
    publish_endpoint:
      type: string

# In specs/domains/publishing/domain.yaml — instantiate:
flows:
  - id: publish-to-twitter
    template: publish-to-platform
    parameters:
      platform: twitter
      api_integration: twitter-api-v2
      publish_endpoint: /2/tweets
  - id: publish-to-linkedin
    template: publish-to-platform
    parameters:
      platform: linkedin
      api_integration: linkedin-api
      publish_endpoint: /v2/ugcPosts
```

---

## 17. Implementation Patterns

This section provides guidance for `/ddd-implement` and other code generators. It maps DDD spec constructs to implementation code artifacts.

### Node Type to Code Artifact Mapping

Each node type produces specific code artifacts. The **primary** artifact is always generated; **secondary** artifacts are generated when the node's spec warrants it.

| Node Type | Primary Artifact | Secondary Artifacts |
|-----------|-----------------|---------------------|
| `trigger` (http) | Route handler / controller | Auth middleware, rate limiter |
| `trigger` (cron) | Scheduled job definition | — |
| `trigger` (event) | Event listener / subscriber | — |
| `input` | Zod validation schema | TypeScript request type |
| `process` | Service function | — |
| `decision` | Conditional branch in service | — |
| `terminal` | Response formatter / return | Error response variant |
| `data_store` (read) | Repository / query function | Prisma query |
| `data_store` (write/update/delete) | Repository / mutation function | Prisma mutation |
| `collection` | Array utility in service | — |
| `service_call` | HTTP client call / SDK wrapper | Retry/timeout config |
| `ipc_call` | IPC message sender / bridge call | Response handler |
| `event` (emit) | Event emitter call | Event type definition |
| `event` (listen) | Event handler registration | — |
| `loop` | `for`/`while` block in service | — |
| `parallel` | `Promise.all` / `Promise.allSettled` | — |
| `sub_flow` | Imported service function call | — |
| `cache` | Cache get/set wrapper | Cache key builder |
| `parse` | Parser utility function | — |
| `crypto` | Crypto utility function | Key management helper |
| `batch` | Batch processor / `Promise.allSettled` loop | Per-item operation handler |
| `transaction` | Database transaction wrapper | Rollback handler |
| `transform` | Data mapping function | — |
| `delay` | `setTimeout` / queue delay | — |
| `llm_call` | LLM client call | Prompt template |
| `agent_loop` | Agentic tool-use loop | Tool definitions |
| `guardrail` | Check middleware / validator | — |
| `human_gate` | Async checkpoint + notification | Approval state persistence |
| `orchestrator` | Strategy dispatcher | Per-strategy handler |
| `smart_router` | Routing function | Confidence scorer |
| `handoff` | Context transfer + agent call | — |
| `agent_group` | Parallel/sequential agent runner | — |

### Service Layer Pattern

Each flow maps to one **service file**. The service exports a main function named after the flow (e.g., `create-order.yaml` → `createOrder()`). Nodes become sequential or branching calls within that function.

```typescript
// services/create-order.service.ts  (generated from create-order.yaml)
import { validateInput } from './validators/create-order.input';
import { orderRepo } from '../repositories/order.repo';

export async function createOrder(ctx: FlowContext): Promise<FlowResult> {
  // input node → validation
  const data = validateInput(ctx.body);

  // process node → business logic
  const order = buildOrder(data);

  // data_store node → persistence
  const saved = await orderRepo.create(order);

  // terminal node → response
  return { status: 201, body: saved };
}
```

**Key rules:**
- One service file per flow, one exported function per flow
- Internal helper functions for process/decision nodes stay private in the file
- Repository functions are shared across flows via the `repositories/` directory
- Schemas and types go in dedicated files, imported by services
- When multiple flows share a domain, group their HTTP routes into a single route file per resource (e.g., `routes/auth.ts` for `user-register` + `user-login`). Each flow's service file stays separate — the route file imports and dispatches to the correct service.

### Error Handling

Connection `sourceHandle` values map to control flow (see Section 8 for the complete handle reference per node type):

| Source Handle | Implementation |
|--------------|----------------|
| (unnamed) | Normal sequential call |
| `"success"` / `"error"` | Success branch / `catch` block (data_store, service_call, ipc_call, parse, crypto, llm_call) |
| `"valid"` / `"invalid"` | Validation pass / rejection (input) |
| `"true"` / `"false"` | `if`/`else` branches (decision) |
| `"hit"` / `"miss"` | Cache lookup branches (cache) |
| `"body"` / `"done"` | Loop iteration body / post-loop (loop) |
| `"branch-N"` / `"done"` | Parallel branches / join point (parallel) |
| `"result"` / `"empty"` | Has results / empty collection (collection) |
| `"pass"` / `"block"` | Validation pass / blocked (guardrail) |
| `"done"` / `"error"` | Completed / failure (agent_loop, batch) |
| `"committed"` / `"rolled_back"` | Transaction outcome (transaction) |
| dynamic IDs | Route IDs (smart_router) or approval option IDs (human_gate) |

Connection `behavior` maps to error handling strategy:

| Behavior | Implementation |
|----------|----------------|
| `continue` | `try { ... } catch(e) { logger.warn(e); }` — log and proceed |
| `stop` | `throw e;` — propagate to flow-level error handler |
| `retry` | Wrap call in retry loop (count from node's `retry.max_attempts` or default 3) |
| `circuit_break` | Use circuit breaker (e.g., `opossum`) with threshold from node spec |

### Middleware and Validation

| Spec Source | Generated Artifact |
|------------|-------------------|
| `input` node `fields[]` with `type` + `required` | Zod schema (`z.object({ ... })`) |
| `input` node `validation` rules | Validation logic (e.g., `z.string().email()`) |
| Node with `security.authentication` | Auth middleware (`requireAuth`, `requireRole`) |
| Node with `security.rate_limiting` | Rate limiter middleware |
| `guardrail` node | Check function called before downstream logic |

```typescript
// validators/create-order.input.ts  (generated from input node)
import { z } from 'zod';

export const CreateOrderInput = z.object({
  product_id: z.string().uuid(),
  quantity: z.number().int().positive(),
  shipping_address: z.string().min(1),
});

export type CreateOrderInput = z.infer<typeof CreateOrderInput>;
```

### Test Generation

Generate tests based on flow structure:

| Flow Element | Test Case |
|-------------|-----------|
| Happy path (trigger → terminal success) | End-to-end success test |
| Each `decision` node | One test per branch (`true` and `false`) |
| Each `error` handle connection | Error path test |
| `input` node with `required` fields | Validation rejection test per field |
| `service_call` node | Mock external call, test success + failure |
| `data_store` node | Mock repository, verify query/mutation |
| `guardrail` node | Test pass and block outcomes |
| `human_gate` node | Test approve and reject paths |

**Test file naming:** `{flow-id}.test.ts` alongside the service file.

### Agent Implementation

| Agent Node | Implementation Pattern |
|-----------|----------------------|
| `agent_loop` | While-loop calling LLM with tool definitions. Each iteration: send messages → get response → if tool_call, execute tool → append result → repeat. Exit on final answer or `max_iterations`. |
| `orchestrator` (supervisor) | Supervisor LLM decides which agent to call next based on current state; loop until done. |
| `orchestrator` (round_robin) | Call each agent in `agents[]` order, passing accumulated context. |
| `orchestrator` (broadcast) | `Promise.all` over `agents[]`, merge results via `result_merge_strategy`. |
| `orchestrator` (consensus) | All agents run, supervisor picks or combines results per `result_merge_strategy`. |
| `smart_router` | Evaluate input against each route's `condition`, pick highest-confidence match, route to that agent. |
| `handoff` | Save current context, call target agent with `context_transfer`. `mode: transfer` (fire-and-forget), `mode: consult` (return result), or `mode: collaborate` (ongoing coordination). |
| `human_gate` | Persist pending state, send notification (email/Slack/webhook per `notification_channels`), expose approval endpoint. On response, resume flow via the matching `approval_options[].id` handle. |
| `agent_group` | Run all agents in group per `coordination.selection_strategy`, collect results, combine via shared memory. |
